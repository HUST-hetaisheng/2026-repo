%%
%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%% !Mode:: "TeX:UTF-8"
%% -----------------------------------
%% This is a generated file.
%% 
%% Copyright (C) 2010 -- 2015 by latexstudio
%%       2014 -- 2019 by Liam Huang
%%       2019 -- present by latexstudio.net
%% 
%% License: The LaTeX Project Public License 1.3c
%% 
%% The Current Maintainer of this work is latexstudio.net.
%% 
\documentclass{mcmthesis}
 %\documentclass[CTeX = true]{mcmthesis}  % 当使用 CTeX 套装时请注释上一行使用该行的设置
\mcmsetup{tstyle=\color{red}\bfseries,%修改题号，队号的颜色和加粗显示，黑色可以修改为 black
        tcn = 2607256, problem = C, %修改队号，参赛题号
        sheet = true, titleinsheet = false, keywordsinsheet = true,%修改sheet显示信息
        titlepage = false, abstract = true}

  %四款字体可以选择
  %\usepackage{times}
  \usepackage{newtxtext,newtxmath} %CTeX 无此字体，可用 txfonts 替代，请使用新版 TeXLive.
  %\usepackage{palatino}
  %\usepackage{txfonts}

\usepackage{indentfirst}  %首行缩进，注释掉，首行就不再缩进。
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\title{The \LaTeX{} Template for MCM Version \MCMversion}
\author{\small \href{https://www.latexstudio.net/}
  {\includegraphics[width=7cm]{mcmthesis-logo}}}
\date{\today}
\begin{document}
\begin{abstract}
\par Use this template to begin typing the first page (summary page) of your electronic report. This
template uses a 12-point Times New Roman font. Submit your paper as an Adobe PDF
electronic file (e.g. 1111111.pdf), typed in English, with a readable font of at least 12-point type. 

Do not include the name of your school, advisor, or team members on this or any page. 

Be sure to change the control number and problem choice above. 

You may delete these instructions as you begin to type your report here.  

\textbf{Follow us @COMAPMath on Twitter or COMAPCHINAOFFICIAL on Weibo for the
most up to date contest information.}

\begin{keywords}
keyword1; keyword2
\end{keywords}
\end{abstract}
\maketitle
%% Generate the Table of Contents, if it's needed.
\tableofcontents
\newpage
%%
%% Generate the Memorandum, if it's needed.
%% \memoto{\LaTeX{}studio}
%% \memofrom{Liam Huang}
%% \memosubject{Happy \TeX{}ing!}
%% \memodate{\today}
%% \memologo{\LARGE I'm pretending to be a LOGO!}
%% \begin{memo}[Memorandum]
%%   \lipsum[1-3]
%% \end{memo}
%%
\section{Introduction}
\subsection{Background}

DWTS (Dancing with the Stars) is a popular television competition where celebrity contestants are paired with professional dancers to compete in weekly dance performances. Elimination and advancement results are determined by combining judges' scores and audience votes. The judges assess technical proficiency, which can be subjective, while fan votes are influenced by factors such as celebrity popularity and charisma. As a result, the competition's outcomes have often sparked discussions and controversies, despite the show's attempts to improve the integration of these two elements. In recent years, with increasing audience concerns about fairness in competitive variety shows, DWTS faces a critical need for a fair, impartial, and effective scoring system. This system must ensure both the program's entertainment value and the confidentiality of fan votes, securing a balance between professionalism and popularity. This is essential for DWTS to maintain its viewership and grow its brand in future seasons.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.35\linewidth]{figures/dwtc背景图.png}
%     \caption{Dancing with the Stars poster}
%     \label{fig:placeholder}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.37\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dwtc背景图.png}
    \end{minipage}
    \hspace{0.05\linewidth}
    \begin{minipage}{0.37\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dwtc问题综述图.png}
    \end{minipage}
    \textbf{\caption{Dancing with the Stars}}
    \label{fig:dwtc_overview}
\end{figure}



% \subsection{Restatement of the Problem}

% Given the background information and available data, this study addresses the following tasks.

% \textbf{Task 1: Fan Vote Estimation.}
% \textbf{1.1}\quad Construct a model to estimate weekly fan votes using judges’ scores, elimination outcomes, and contestant data.  
% \textbf{1.2}\quad Evaluate the consistency of the estimates by testing whether they reproduce the observed weekly eliminations.  
% \textbf{1.3}\quad Quantify the uncertainty of the estimated fan votes and examine whether it varies across contestants or weeks.

% \textbf{Task 2: Comparison of Vote Combination and Elimination Rules.}
% \textbf{2.1}\quad Compare the rank-based and percentage-based vote combination methods across seasons and analyze potential biases.  
% \textbf{2.2}\quad Evaluate the impact of alternative elimination procedures, including judges selecting from the bottom two contestants.  
% \textbf{2.3}\quad Recommend an appropriate combination and elimination approach for future seasons with justification.

% \textbf{Task 3: Impact of Contestant and Partner Characteristics.}
% \textbf{3.1}\quad Assess the influence of professional dancer traits and celebrity characteristics on overall performance and final results.  
% \textbf{3.2}\quad Compare the effects of these factors on judges’ scores versus fan votes.

% \textbf{Task 4: Alternative Scoring System.}
% \textbf{4.1}\quad Design an alternative method for combining judges’ scores and fan votes.  
% \textbf{4.2}\quad Demonstrate that the proposed system is more fair or otherwise improves the competition.
\subsection{Restatement of the Problem}

Given the background information and available data, this study addresses the following tasks.

\textbf{Task 1: Fan Vote Estimation.}\quad
A mathematical model is developed to estimate weekly fan vote totals for each contestant using judges’ scores, elimination outcomes, and contestant data. The model’s consistency is evaluated by its ability to reproduce observed weekly eliminations, and the uncertainty of the estimated fan votes is quantified and analyzed across contestants and weeks.

\textbf{Task 2: Comparison of Vote Combination and Elimination Rules.}\quad
Using the estimated fan votes, the rank-based and percentage-based methods for combining judges’ scores and fan votes are compared across seasons. The effects of alternative elimination procedures, including judges selecting from the bottom two contestants, are also evaluated, leading to a justified recommendation for future seasons.

\textbf{Task 3: Impact of Contestant and Partner Characteristics.}\quad
The impacts of professional dancer traits and celebrity characteristics on competition outcomes are analyzed, with a comparison of their influences on judges’ scores and fan votes.

\textbf{Task 4: Alternative Scoring System.}
An alternative scoring system combining \quad
judges’ scores and fan votes is proposed and evaluated to determine whether it improves fairness or other aspects of the competition.


\section{Preparations for Modeling}

\subsection{Model Assumptions}

\textbf{Assumption 1 (Vote Share Modeling):}
Fan votes are modeled as relative vote shares rather than absolute counts. For each week, all contestants’ vote shares are nonnegative and sum to one.

\textbf{Assumption 2 (Temporal Smoothness):}
For contestants remaining in the competition, fan vote shares change smoothly between consecutive weeks.

\textbf{Assumption 3 (Maximum Entropy):}
In the absence of restrictive information, fan vote distributions are assumed to be as uniform as possible.

\textbf{Assumption 4 (Judges’ Save Bias):}
When judges choose between the bottom two contestants, they are more likely to save the contestant with the higher judges’ score, but the decision is not deterministic.

\textbf{Assumption 5 (Stability to Perturbations):}
Small perturbations in judges’ scores produce bounded changes in the estimated fan vote shares.

\textbf{Assumption 6 (Independent Perturbations):}
Judges’ score perturbations are assumed to be independent across contestants and weeks.


\subsection{Notations}
\label{sec:notations}

We use the following notation throughout the paper:

\begin{table}[H]
\centering
\caption{Key Notations and Symbols}
\begin{tabular}{cl}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$i$ & Contestant index \\
$t$ & Week index (1 to $T$) \\
$s$ & Season index (1 to 34) \\
$J_{i,t}$ & Total judges' score for contestant $i$ in week $t$ \\
$j_{i,t}$ & Judges' score share: $j_{i,t} = J_{i,t} / \sum_{k\in A_t} J_{k,t}$ \\
$f_{i,t}$ & Fan vote share for contestant $i$ in week $t$ (estimated) \\
$V_{i,t}$ & Absolute fan vote count: $V_{i,t} = T_t \cdot f_{i,t}$ \\
$T_t$ & Total votes cast in week $t$ (set to $10^7$ for presentation) \\
$c_{i,t}$ & Combined score: $c_{i,t} = j_{i,t} + f_{i,t}$ (Percent regime) \\
$r^J_{i,t}$ & Judges' rank for contestant $i$ in week $t$ (Rank regime) \\
$r^F_{i,t}$ & Fan vote rank for contestant $i$ in week $t$ (Rank regime) \\
$A_t$ & Active contestant set in week $t$: $A_t = \{i : J_{i,t} > 0\}$ \\
$E_t$ & Eliminated contestant set in week $t$ (vote-determined) \\
$S_t$ & Safe contestant set in week $t$: $S_t = A_t \setminus E_t$ \\
$\delta_t$ & Slack variable for week $t$ (relaxes hard constraints) \\
$\alpha, \beta, \gamma$ & Regularization hyperparameters \\
$M$ & Large penalty coefficient for slack minimization \\
$K$ & Number of ensemble runs for uncertainty quantification \\
\bottomrule
\end{tabular}
\label{tab:notations}
\end{table}

\subsection{Data Preprocessing}
\label{sec:data_processing}

The raw dataset covers 34 seasons and includes 421 celebrity--partner pairs with weekly judges' scores and elimination outcomes. Several preprocessing steps are applied to prepare the data for inverse modeling.

\begin{itemize}
\item \textbf{Missing and placeholder values.}  
In the original dataset, zero values indicate either judge absences or post-elimination weeks. A contestant-week is treated as \emph{active} if at least one judge score is positive. For each active observation, the total judges' score $J_{i,t}$ is computed as the sum of all non-zero individual scores. Elimination outcomes are extracted from the \texttt{results} column. Contestants labeled as ``Withdrew'' are flagged separately, as their exits are not vote-determined and thus impose no constraints on fan vote estimation.

\item \textbf{Rule regime classification.}  
Each season is classified according to the competition rules in effect \cite{WikiDWTS}: \emph{Rank} (Seasons~1--2), where judges' and fan rankings are combined; \emph{Percent} (Seasons~3--27), where normalized score shares are added; and \emph{Bottom2} (Seasons~28--34), where judges select which contestant to save from the bottom two. This classification determines the form of elimination constraints used in the inverse model.

\item \textbf{Elimination pattern annotation.}  
Each week is labeled by its elimination structure, including single elimination, double elimination, no elimination (results shows), and finals. For finals weeks, the \texttt{placement} column is used to determine the final ranking among remaining contestants.

\item \textbf{Data validation.}  
We verify that eliminations are monotonic (no contestant reappears after elimination), that finals rankings are consistent with the final active set, and that all judge scores lie within the valid 1--10 range.
\end{itemize}

After preprocessing, the cleaned dataset contains 2,699 valid contestant-week observations with complete judges' scores and unambiguous elimination status.
\section{The Entropy-Inertia Regularized Inverse Optimization Model}

\subsection{Rationale and Overall Architecture}
The central challenge of this problem is that fan votes ($\mathbf{v}_t$) are unobserved latent variables, while only the weekly elimination results ($E_{obs}$) are known. This constitutes an \textbf{ill-posed inverse problem} with incomplete information: a single elimination event determines the worst-performing contestant but leaves the relative ordering of the survivors underdetermined. 

To reconstruct the fan votes robustly, a valid model must: (i) \textbf{Respect Historical Regimes} by adapting to the specific aggregation rules (Rank vs. Percent) that evolved over 34 seasons; (ii) \textbf{Handle Irregularities} such as no-elimination weeks, double eliminations, and withdrawals; and (iii) \textbf{Avoid Bias}. Unlike naive approaches that force fan votes to correlate with judge scores, we adopt an \textbf{unbiased} approach that only assumes fundamental statistical properties: entropy and temporal inertia.

Therefore, we propose a \textbf{Regime-Specific, Week-Type Aware Inverse Optimization Framework}. We classify the problem into three distinct mathematical sub-models corresponding to the show's eras: \textit{The Rank Regime} (S1--2), \textit{The Percent Regime} (S3--27), and \textit{The Judges' Choice Regime} (S28--34).

\subsection{Week-Type Identification}
The transformation from observed elimination results to latent fan votes represents an ill-posed inverse problem. To restrict the feasible solution space effectively, we must adapt our optimization constraints dynamically. Different competitive scenarios imply varying degrees of logical restrictiveness. Accordingly, we identify five distinct week archetypes to tailor the constraint set $\Omega_w$:

\begin{itemize}
    \item[$\star$] \textbf{Single Elimination ($|E_w|=1$):} The standard case. A strict inequality constraint separates the eliminated contestant from all survivors.
    \item[$\star$] \textbf{Multiple Elimination ($|E_w|>1$):} All eliminated contestants must have lower scores than any survivor.
    \item[$\star$] \textbf{No Elimination ($E_w=\varnothing$):} No hard constraints are imposed on the lower bound, but temporal smoothness still guides the evolution of votes.
    \item[$\star$] \textbf{Withdrawal:} Treated as a non-informative event for the withdrawn star; constraints apply only to the remaining pool if a vote-based elimination still occurred.
    \item[$\star$] \textbf{Finals:} The final placement $\{p_1, p_2, \dots\}$ provides a total or partial ordering, imposing the strictest constraints.
\end{itemize}

% =================================================================================
% MODEL 1: PERCENT REGIME (The Convex Optimization Core)
% =================================================================================
\subsection{The Percent Regime Model (Seasons 3--27)}
\label{sub:percent_model}

\subsubsection{Mathematical Formulation}
In these seasons, the judge score share $j_{i,w}$ and fan vote share $f_{i,w}$ are additive. The total combined score $S_{i,w}$ is defined as $S_{i,w} = j_{i,w} + f_{i,w}$. Since the variables are continuous and the constraints are linear, this yields a \textbf{Convex Optimization Problem}.

\subsubsection{The Unbiased Objective Function (MaxEnt + Inertia)}
We explicitly reject the inclusion of a "Judge Consensus" term to avoid confirmation bias. Instead, we minimize a composite objective function $\mathcal{L}(f_w)$ grounded in Information Theory:

\begin{equation}
    \min_{f_w} \quad \mathcal{L}(f_w) = \underbrace{\sum_{i} f_{i,w} \ln f_{i,w}}_{\text{Negative Entropy}} + \lambda \underbrace{\sum_{i} f_{i,w} \ln \left( \frac{f_{i,w}}{f_{i,w-1}} \right)}_{\text{Temporal Inertia (KL Divergence)}} + \rho \mathcal{L}_{place}
\end{equation}

The first term, \textbf{Negative Entropy}, ensures the distribution is as uniform as possible unless the data (elimination constraints) forces otherwise. This prevents the model from assuming arbitrary popularity spikes. The second term, \textbf{Temporal Inertia}, utilizes Kullback-Leibler divergence to penalize deviation from the previous week's posterior, reflecting the sociological "stickiness" of fan bases.

Finally, if partial placement data is available (e.g., "Contestant A was 3rd, B was 4th" among eliminated), we add a \textbf{Hinge Loss} soft penalty $\mathcal{L}_{place} = \sum_{(a,b) \in \mathcal{P}_w} \max\left(0, S_{b,w} - S_{a,w} + \epsilon \right)$, where $\mathcal{P}_w$ is the set of known pairs where $a$ ranked better than $b$.

\subsubsection{Hard Constraints}
The feasible region is defined by the elimination event. Let $E_w$ be the set of eliminated contestants and $S_{safe}$ be the set of survivors. The constraint $S_{e,w} \le S_{s,w}$ ($\forall e \in E_w, \forall s \in S_{safe}$) forms a convex polytope. We solve this using the \textbf{Sequential Least Squares Programming (SLSQP)} algorithm, guaranteeing a global optimum.

% =================================================================================
% MODEL 2: RANK REGIME (The Discrete Combinatorial Core)
% =================================================================================
\subsection{The Rank Regime Model (Seasons 1--2)}

\subsubsection{The Discrete Challenge}
In rank-based seasons, inputs are ordinal integers $r \in \{1, \dots, N\}$. The combined score is the sum of ranks $R_{total, i} = r^J_{i} + r^F_{i}$, where $r^F$ is a permutation vector. The inverse problem becomes a \textbf{Combinatorial Optimization} task (NP-Hard nature), prohibiting gradient-based methods.

\subsubsection{Greedy Heuristic Search Algorithm}
Instead of brute-force enumeration ($N!$ complexity), we employ a \textbf{Stochastic Greedy Swapping Heuristic}. The algorithm proceeds in three steps:
\textbf{First}, it initializes with a fan rank permutation $r^F$ correlated with judge ranks to ensure a valid starting point.
\textbf{Second}, it validates the elimination rule: $\max_{e \in E_w} (R_{total, e}) \ge \max_{s \in S_{safe}} (R_{total, s})$.
\textbf{Third}, if the constraint is violated (i.e., the actual eliminated star has a "better" score than a survivor), the algorithm performs minimal swaps between the survivor and the eliminated star until the condition holds, prioritizing permutations that minimize the Kendall-Tau distance from the previous week.

\subsubsection{Rank-to-Share Mapping (Gibbs Distribution)}
To compare these results with Percent seasons, we map the inferred discrete ranks $r^F_i$ to continuous shares $f_i$ using a Softmax distribution:
\begin{equation}
    f_{i} = \frac{\exp(-\gamma \cdot (r^F_{i} - 1))}{\sum_k \exp(-\gamma \cdot (r^F_{k} - 1))}
\end{equation}
where $\gamma$ is solved using data from the third season, then back-substituted into the current stage's problem solution.

% =================================================================================
% MODEL 3: JUDGES' SAVE REGIME (The Probabilistic Extension)
% =================================================================================
\subsection{The Judges' Choice Model (Seasons 28--34)}

Starting from Season 28, the competition introduced a significant procedural discontinuity: the "Judges' Save." Under this regime, the combined judge and fan votes determine a "Bottom Two" (or occasionally Bottom Three) pool, from which the judges programmatically select one contestant to eliminate. This mechanism breaks the deterministic "lowest score eliminated" rule, as the person actually eliminated might not be the one with the absolute minimum total support.

\subsubsection{Constraint Relaxation: The Bottom-Two Set}
To reflect this mechanism, we relax the hard elimination constraint used in earlier seasons. Instead of requiring the eliminated contestant $e$ to possess the minimum combined score, we define a \textbf{Set-Membership Constraint}. Let $R_{i,w}$ be the combined rank of contestant $i$ in week $w$, where $R_{i,w} = r^J_{i,w} + r^F_{i,w}$ (with higher values indicating worse performance). The feasible space $\Omega_w$ is defined such that the eliminated contestant must belong to the bottom $K$ of the field:
\begin{equation}
    e \in \text{Bottom}_K(\mathbf{R}_w) \iff \text{Rank}(R_{e,w}) \ge N - K + 1
\end{equation}
In our implementation, we set $K=2$ to align with the standard "Bottom Two" format. This relaxation acknowledges that the eliminated individual was \textit{at risk}, but their final exit was moderated by judicial discretion.

\subsubsection{Solution Algorithm: Stochastic Greedy Swapping}
Since the rank-based combined score is discrete and non-differentiable, we employ a \textbf{Greedy Swapping Heuristic} (as implemented in our \texttt{enforce\_elimination\_rank} function) to find a feasible fan ranking $\mathbf{r}^F_w$. 
$\star$ \textbf{Initialization:} The fan ranks are initially set to a prior distribution (e.g., the previous week's ranks or judge-correlated ranks).
$\star$ \textbf{Violation Identification:} If the observed eliminated contestant $e$ is not
within the $\text{Bottom}_2$ of the combined ranks $\mathbf{R}_w$, the algorithm identifies
\emph{offenders}—contestants who currently occupy the bottom two but should be safer than $e$.

$\star$ \textbf{Minimal Swap Logic:} The algorithm iteratively executes minimal rank swaps between $e$ and these offenders. Each swap is designed to push $e$ deeper into the danger zone while minimizing the Kendall-Tau distance from the prior ranking, ensuring the reconstructed votes represent the \textit{least-informed sufficient deviation} required to explain the result.

\subsubsection{Probabilistic Mapping and Ensemble Inference}
To translate these discrete ranks into continuous vote shares for cross-season analysis, we apply a \textbf{Gibbs-Softmax Mapping}:
\begin{equation}
    f_{i,w} = \frac{\exp(-\gamma(r^F_{i,w}-1))}{\sum_{k} \exp(-\gamma(r^F_{k,w}-1))}
\end{equation}
where $\gamma$ captures the latent "decay" of fan support across the leaderboard. 

To quantify the inherent ambiguity of the Judges' Save, we do not produce a single deterministic answer. Instead, we utilize an \textbf{Ensemble Perturbation Method}. By injecting Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$ into the judge scores $\mathbf{J}_w$ and running the swapping heuristic $N=1,000$ times, we generate a posterior distribution of $f_{i,w}$. 

The "Judges' Save" logic is implicitly captured here: if a contestant with high judge scores is eliminated, the ensemble of valid $f_{i,w}$ will converge toward significantly lower values to counteract the high $J_{i,w}$ and force them into the Bottom Two. The variance of this ensemble (Coefficient of Variation) serves as our formal measure of model certainty, reflecting the "transparency" of the elimination in that specific week.
\subsection{Model Performance and Empirical Validation}

To address the core questions of consistency and certainty in our fan vote estimates, we evaluate model performance along two orthogonal dimensions: (1)~\textit{consistency}---whether our inferred fan votes produce elimination outcomes that match the observed results; and (2)~\textit{uncertainty}---how precisely we can pinpoint the fan vote shares, quantified by the coefficient of variation (CV) and confidence interval width.

\subsubsection{Consistency Analysis}

\textbf{Definition.} For each week $w$, let $\hat{v}_i^{(w)}$ denote our estimated fan vote share for contestant $i$, and let $E_w$ be the set of contestants actually eliminated. We define the \textit{consistency probability} as the proportion of weeks where the model's predicted bottom-ranked contestant(s) under the applicable voting rule coincide with $E_w$:
\begin{equation}
    P_{\text{cons}} = \frac{1}{|W|} \sum_{w \in W} \mathbf{1}\left[\text{Elim}(\hat{\mathbf{v}}^{(w)}) = E_w\right].
\end{equation}

\textbf{Results by Season.} As illustrated in Figure~\ref{fig:cons_cer} (upper-right panel), the model achieves near-perfect consistency across the Percent regime (Seasons~3--27), with an average consistency probability of $\bar{P}_{\text{cons}} = 0.95$. The Rank regime (Seasons~1--2) exhibits slightly lower consistency ($\bar{P} = 0.88$), attributable to the rank-compression effect that reduces the discriminative power of elimination signals. The Bottom-2 regime (Seasons~28--34) maintains robust consistency ($\bar{P} = 0.89$) despite the structural ambiguity introduced by the judges' save mechanism.

\textbf{Results by Week.} The lower-right panel of Figure~\ref{fig:cons_cer} reveals a temporal pattern: consistency remains high ($P > 0.93$) during early-to-mid competition (Weeks~1--8), but declines sharply in later weeks---particularly Week~10 ($P = 0.55$) and Week~11 ($P = 0.56$). This phenomenon reflects the \textit{information scarcity} in late-stage competition: with only 3--4 contestants remaining, a single elimination event provides limited discriminative constraints, increasing the feasible region of fan vote distributions that could explain the outcome.

\subsubsection{Uncertainty Quantification}

\textbf{Metrics.} We employ two complementary measures of inference precision:
\begin{itemize}
    \item \textbf{Coefficient of Variation (CV)}: $\text{CV}_i = \sigma_i / \mu_i$, capturing relative volatility of estimates across Monte Carlo samples.
    \item \textbf{Confidence Interval Width}: The 95\% CI width $w_{95} = \hat{v}_i^{(97.5)} - \hat{v}_i^{(2.5)}$, measuring absolute uncertainty magnitude.
\end{itemize}

\textbf{Regime-Dependent Uncertainty.} Table~\ref{tab:uncertainty} and Figure~\ref{fig:cons_cer} (left panels) reveal a striking regime-dependent pattern in model uncertainty:

\begin{itemize}
    \item \textbf{Percent Regime (S3--27):} The model achieves exceptional precision with an average CV of only $\bar{\text{CV}} = 0.014$ (1.4\%). This reflects the rich informational content of percentage-based voting: the exact numerical thresholds for elimination impose tight mathematical constraints on feasible fan vote distributions.
    
    \item \textbf{Rank Regime (S1--2):} Uncertainty is elevated ($\bar{\text{CV}} = 0.186$), representing a 13$\times$ increase over the Percent era. The rank-transformation compresses continuous vote shares into discrete ordinal positions, destroying magnitude information and widening the feasible solution space.
    
    \item \textbf{Bottom-2 Regime (S28--34):} The highest uncertainty ($\bar{\text{CV}} = 0.449$) arises from the judges' save mechanism. Since judges may override pure fan-vote rankings, the elimination outcome no longer deterministically identifies the lowest-voted contestant, introducing fundamental ambiguity that propagates through our inference.
\end{itemize}

\textbf{Temporal Evolution.} The spatio-temporal uncertainty visualization (Figure~\ref{fig:cons_cer}, lower-left) demonstrates two key patterns:

\begin{enumerate}
    \item \textbf{Within-Season Convergence:} For the Rank regime, CV decreases monotonically from Week~1 (0.29) to Week~8 (0.08), reflecting the cumulative information gain as sequential elimination events progressively constrain the feasible fan vote space.
    
    \item \textbf{Late-Week Divergence:} Paradoxically, CI width \textit{increases} in final weeks (see Figure~\ref{fig:cons_cer}, upper-left). With fewer contestants remaining, each individual's vote share becomes more sensitive to perturbations, and the constraint structure becomes sparser, leading to an ``uncertainty horizon'' effect.
\end{enumerate}

\subsubsection{Summary of Validation Findings}

\begin{table}[htbp]
\centering
\caption{Summary of Model Performance Metrics by Voting Regime}
\label{tab:validation_summary}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Rank (S1--2)} & \textbf{Percent (S3--27)} & \textbf{Bottom-2 (S28--34)} \\
\midrule
Avg Consistency Prob. & 0.88 & 0.95 & 0.89 \\
Avg CV & 0.186 & 0.014 & 0.449 \\
Avg CI Width (95\%) & 0.091 & 0.008 & 0.118 \\
Information Quality & Moderate & High & Low \\
\bottomrule
\end{tabular}
\end{table}

In conclusion, our model demonstrates strong empirical validity: it correctly recovers elimination outcomes with high probability (overall $\bar{P}_{\text{cons}} = 0.93$), while uncertainty quantification reveals interpretable regime-dependent patterns that align with the theoretical information content of each voting mechanism. The Percent regime provides the most reliable inference, while the judges' save mechanism in recent seasons introduces irreducible ambiguity that is faithfully captured by our probabilistic framework.

% -------------------- 引用图片 --------------------
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/cons&cer.png}
    \caption{Comprehensive visualization of model consistency and uncertainty. \textit{Upper-left:} Temporal evolution of inference confidence (CV and CI width by week). \textit{Lower-left:} Spatio-temporal CI width evolution across 34 seasons. \textit{Upper-right:} Weekly consistency trends by voting regime. \textit{Lower-right:} Season-level consistency probability with regime-colored bars.}
    \label{fig:cons_cer}
\end{figure}

\section{Model Development}

\subsection{Rationale and Overall Structure}
The central difficulty is that fan votes are unobserved while only weekly eliminations are known. 
This is an inverse problem with incomplete information: a single elimination only determines 
the worst-ranked contestant but leaves the relative ordering of the remaining contestants 
underdetermined. Hence, a valid model must (i) respect the show’s rule changes across seasons, 
(ii) handle irregular weekly events (no elimination, multiple eliminations, withdrawals, finals), 
and (iii) introduce principled regularization to obtain stable, interpretable solutions.
Therefore we adopt a \emph{regime-specific, week-type–aware} inverse framework: 
Rank-based seasons (1--2), Percent-based seasons (3--27), and Rank with bottom-two + judges’ save 
(28--34). This structure directly mirrors the show’s mechanics, avoids rule-mismatch bias, 
and provides a transparent path to evaluate consistency and uncertainty.


\subsection{Week-Type Identification}
Each week is classified as:
\begin{itemize}
  \item \textbf{Single elimination}: $|E_w|=1$.
  \item \textbf{Multiple elimination}: $|E_w|>1$.
  \item \textbf{No elimination}: $E_w=\varnothing$.
  \item \textbf{Withdrew}: a contestant exits externally; no vote-based constraint is imposed.
  \item \textbf{Finals}: placement determines the final ordering.
\end{itemize}
This classification is critical because elimination constraints are only valid when the show
actually used a vote-based decision.

\subsection{Percent Regime (Seasons 3--27)}
\textbf{Why this model?} In percent seasons, judges' and fan contributions are combined 
via normalized shares. Thus a linear combined score is the most faithful reconstruction of 
the production rule, and it yields a convex inverse problem when paired with regularization.

\paragraph{Combined score.}
\begin{equation}
S_{i,w}=j_{i,w}+f_{i,w}.
\label{eq:percent_score}
\end{equation}

\paragraph{Elimination constraints (hard).}
For single elimination,
\begin{equation}
S_{e,w}\le S_{i,w},\quad \forall i\in A_w\setminus\{e\}.
\label{eq:percent_single}
\end{equation}
For multiple elimination,
\begin{equation}
S_{e,w}\le S_{i,w},\quad \forall e\in E_w,\ \forall i\notin E_w.
\label{eq:percent_multi}
\end{equation}

\paragraph{Placement usage (soft, within-week only).}
Placement is a season-level outcome, so we avoid using it across different weeks.
If $a,b\in E_w$ and $p_a>p_b$ (worse final placement), we add a soft penalty:
\begin{equation}
\mathcal{L}_{\text{place}}=\sum_{(a,b)\in\mathcal{P}_w}\big[\,S_{a,w}-S_{b,w}\,\big]_+,
\quad \mathcal{P}_w=\{(a,b)\mid a,b\in E_w,\ p_a>p_b\}.
\label{eq:percent_place_soft}
\end{equation}
When placements are identical, no internal ordering is imposed.

\paragraph{Regularized inverse objective.}
To resolve non-identifiability (especially in week 1 or no-elimination weeks), we solve:
\begin{equation}
\min_{f_w}\ \lambda_1\|f_w-f_{w-1}\|_2^2+\lambda_2\|f_w-j_w\|_2^2
+\rho\,\mathcal{L}_{\text{place}}
\label{eq:percent_objective_soft}
\end{equation}
subject only to the simplex and elimination constraints. 
The first term enforces temporal smoothness; the second aligns votes with dance quality.

\subsection{Rank Regime (Seasons 1--2)}
\textbf{Why this model?} Rank seasons only reveal relative order, not score magnitudes.
Thus the natural inverse object is the fan rank, not a numeric score. 
We infer a feasible fan ranking consistent with eliminations, and map it to shares.

\paragraph{Combined rank.}
\begin{equation}
R_{i,w}=r^J_{i,w}+r^F_{i,w},\quad \text{(larger is worse)}.
\label{eq:rank_score}
\end{equation}

\paragraph{Elimination constraints (hard).}
Single elimination requires $R_{e,w}=\max_i R_{i,w}$.
Multiple elimination requires $E_w$ to be the $|E_w|$ worst combined ranks.

\paragraph{Placement within elimination set.}
If $a,b\in E_w$ and $p_a>p_b$, enforce:
\begin{equation}
R_{a,w}\ge R_{b,w}.
\label{eq:rank_place}
\end{equation}

\paragraph{Minimal-change fan ranking.}
Among all feasible permutations of $r^F_{i,w}$, we choose the one with minimum change from the
previous week (or closest to judges’ ranks in week 1). This is the weakest informative prior
that yields a stable solution while preserving the ranking rule.

\paragraph{Rank-to-share mapping.}
\begin{equation}
f_{i,w}\propto \exp\{-\gamma(r^F_{i,w}-1)\},
\label{eq:rank_map}
\end{equation}
with normalization $\sum_i f_{i,w}=1$.

\subsection{Bottom-Two + Judges’ Save (Seasons 28--34)}
\textbf{Why this model?} The show identifies a bottom-two using combined ranks, then judges select
which to eliminate. A deterministic “lowest score eliminated” rule would misrepresent this
mechanism and bias inference. We therefore model the judges’ save as a probability depending on
judges’ scores.

\paragraph{Bottom-two constraint (hard).}
\begin{equation}
e\in \text{Bottom2}(R_{i,w}).
\label{eq:bottom2}
\end{equation}

\paragraph{Judges’ choice probability (behavioral).}
For bottom-two contestants $a,b$:
\begin{equation}
P(\text{elim}=a)=\frac{\exp(-\eta J_a)}{\exp(-\eta J_a)+\exp(-\eta J_b)}.
\label{eq:judges_save}
\end{equation}
Higher judges’ scores imply lower elimination probability. If a high-scoring contestant is
eliminated, the model attributes this to relatively low fan support.

\paragraph{Placement usage.}
Placement is used only to order contestants within the same elimination set (as in \eqref{eq:rank_place}),
never to constrain other weeks.

\subsection{Finals}
Finals provide the strongest information: the combined rule directly ranks remaining contestants.
Thus we impose a hard ordering in finals weeks only:
\[
\text{placement}(i)<\text{placement}(j)\Rightarrow 
\begin{cases}
S_{i,w}\ge S_{j,w} & \text{(Percent)}\\
R_{i,w}\le R_{j,w} & \text{(Rank)}
\end{cases}
\]

\subsection{Pseudocode}
% =============================================================================
% Fan Vote Inversion - Pseudocode (algorithm + algpseudocode)
% Required: \usepackage{algorithm}, \usepackage{algpseudocode}
% =============================================================================

% --- Brief Description ---
% The inverse problem is solved by a regime-specific approach:
% (1) Percent regime (Seasons 3--27): convex optimization with elimination constraints;
% (2) Rank/Bottom2 regime (Seasons 1--2, 28--33): discrete rank assignment with iterative enforcement.
% Uncertainty is quantified via ensemble perturbation of hyperparameters and judges' scores.

\begin{algorithm}[H]
\caption{Fan Vote Inversion Framework}
\label{alg:main}
\begin{algorithmic}[1]
\Require Dataset $\mathcal{D}$, hyperparameters $\lambda_1, \lambda_2, \rho, \gamma$
\Ensure Fan vote shares $\{f_{i,w}\}$, consistency indicators $\{C_w\}$
\For{each season $s$ and week $w$}
    \State $\mathcal{R} \gets \textsc{GetRegime}(s)$; \quad $(A_w, E_w, J_w) \gets \textsc{ExtractData}(s,w)$
    \If{$\mathcal{R} = \texttt{percent}$}
        \State $f_w \gets \textsc{SolvePercent}(J_w, A_w, E_w, f_{w-1})$ \Comment{Alg.~\ref{alg:percent}}
    \Else
        \State $r^F_w \gets \textsc{AssignRanks}(J_w, E_w, r^F_{w-1}, \mathcal{R})$ \Comment{Alg.~\ref{alg:rank}}
        \State $f_w \gets \exp(-\gamma(r^F_w - 1)) / \sum \exp(\cdot)$ \Comment{Rank-to-share}
    \EndIf
    \State $C_w \gets \textsc{CheckConsistency}(f_w, J_w, E_w, \mathcal{R})$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Percent Regime Solver (Seasons 3--27)}
\label{alg:percent}
\begin{algorithmic}[1]
\Require $J_w, A_w, E_w, f_{w-1}, \lambda_1, \lambda_2, \rho$
\Ensure Fan vote shares $f_w$
\State $j_w \gets J_w / \sum J_w$ \Comment{Normalize judges' shares}
\State \textbf{Objective:} $\min_f \lambda_1\|f - f_{w-1}\|^2 + \lambda_2\|f - j_w\|^2 + \rho \cdot \textsc{OrderPenalty}$
\State \textbf{Constraints:} $\sum_i f_i = 1$, $f_i \ge 0$, $S_i \ge S_e\ (\forall i \notin E_w, e \in E_w)$
\State $f^{(0)} \gets \textsc{FeasibleLP}(j_w, E_w)$; \quad $f_w \gets \textsc{SLSQP}(f^{(0)})$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Rank Regime Solver (Seasons 1--2, 28--33)}
\label{alg:rank}
\begin{algorithmic}[1]
\Require $J_w, E_w, r^F_{w-1}, \mathcal{R}$
\Ensure Fan ranks $r^F_w$
\State $r^J_w \gets \textsc{DenseRank}(-J_w)$; \quad $\pi \gets \textsc{SortBy}(r^F_{w-1}$ or $r^J_w)$
\State Assign worst ranks to $E_w$; fill remaining ranks by order $\pi$
\If{$\mathcal{R} = \texttt{bottom2}$}
    \State Find partner $p = \arg\max_{i \neq e} J_i$ s.t. $J_i \ge J_e$; set $r^F_p = n-1$
\EndIf
\State \textbf{Enforce:} Swap ranks until $E_w \subseteq \arg\max_k(r^J + r^F)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Ensemble Uncertainty Quantification}
\label{alg:ensemble}
\begin{algorithmic}[1]
\Require Base parameters, ensemble size $M$
\Ensure $\mu, \sigma, \text{CI}_{95\%}$ for each $f_{i,w}$
\For{$m = 1$ to $M$}
    \State Perturb: $\tilde{\lambda} \sim \lambda(1 + \mathcal{U}_{[-0.3,0.3]})$, $\tilde{J} \sim J + \mathcal{N}(0, 0.5)$
    \State Run inversion with perturbed parameters $\to f^{(m)}$
\EndFor
\State $\mu_{i,w} \gets \text{Mean}(\{f^{(m)}\})$, $\sigma_{i,w} \gets \text{Std}$, $\text{CI} \gets [Q_{2.5\%}, Q_{97.5\%}]$
\end{algorithmic}
\end{algorithm}

\subsection{Consistency Metrics: Principle and Computation}
\textbf{Principle.} Consistency measures whether the inferred fan votes reproduce the weekly
elimination outcomes under the correct voting regime, without using future information.
We only use \texttt{placement} for (i) finals-week ordering, and (ii) internal ordering
within the \emph{same} multi-elimination set.

\textbf{Weekly indicator.} For each week $w$ we define a binary indicator $C_w$:
\begin{itemize}
  \item \textbf{Percent/Rank regimes:} $C_w=1$ if the predicted elimination set matches the observed
  elimination set $E_w$; otherwise $C_w=0$.
  \item \textbf{Bottom-2 regime:} $C_w=1$ if the observed eliminated contestant lies in the model’s
  bottom-two set; for multi-elimination, $C_w=1$ if the observed elimination set is contained in the
  worst-$|E_w|$ positions.
  \item \textbf{Finals week:} $C_w=1$ if the final ordering implied by $S_{i,w}$ (Percent) or
  $R_{i,w}$ (Rank) matches \texttt{placement}.
  \item \textbf{No-elimination / Withdrew weeks:} we set $C_w=1$ (no elimination is consistent by definition).
\end{itemize}

\textbf{Internal ordering for multi-elimination.}
If $a,b\in E_w$ and $p_a>p_b$ (worse placement), we require
$S_{a,w}\le S_{b,w}$ (Percent) or $R_{a,w}\ge R_{b,w}$ (Rank) within the elimination set.
This does \emph{not} constrain survivors and therefore does not leak future information.

\textbf{Season and week aggregates.}
\[
\text{CSR}_s=\frac{1}{|W_s|}\sum_{w\in W_s}C_w,\qquad
\text{CSR}_{\text{week}}(t)=\frac{1}{|S_t|}\sum_{s\in S_t}C_{s,t}.
\]
These form the inputs for Table~\ref{tab:consistency_by_season} and
Table~\ref{tab:consistency_by_week} (and their corresponding figures).

\subsection{Uncertainty Metrics: Principle and Computation}
\textbf{Principle.} Because elimination outcomes do not uniquely identify fan votes,
we quantify uncertainty by perturbing model hyperparameters and judges’ scores to
generate an ensemble of feasible solutions.

\textbf{Ensemble statistics (per contestant-week).}
For each $(i,w)$, let $\{f^{(m)}_{i,w}\}$ be the ensemble samples. We compute:
\[
\hat{\mu}_{i,w}=\frac{1}{M}\sum_m f^{(m)}_{i,w},\quad
\hat{\sigma}_{i,w}=\sqrt{\frac{1}{M-1}\sum_m(f^{(m)}_{i,w}-\hat{\mu}_{i,w})^2},
\]
\[
\text{CV}_{i,w}=\frac{\hat{\sigma}_{i,w}}{\hat{\mu}_{i,w}},\quad
CI_{i,w}=[Q_{2.5\%},Q_{97.5\%}].
\]

\textbf{Aggregations.}
We summarize uncertainty by week and by season:
\[
\overline{\text{CV}}_s=\frac{1}{N_s}\sum_{(i,w)\in s}\text{CV}_{i,w},\qquad
\overline{\text{CV}}_t=\frac{1}{N_t}\sum_{(i,w)\in t}\text{CV}_{i,w},
\]
with analogous averages for CI width. These are reported in
Table~\ref{tab:uncertainty_by_season} and Table~\ref{tab:uncertainty_by_week}.

\subsection{Interpretation of Consistency and Uncertainty}
\textbf{Consistency.} As shown in Figure~\ref{fig:consistency_by_season} and
Figure~\ref{fig:consistency_by_week}, the model achieves very high consistency in
Percent seasons, because numeric score constraints sharply restrict feasible vote distributions.
Bottom-2 seasons exhibit lower consistency and larger variability, reflecting the
judges’ save mechanism that weakens direct inference from public votes.

\textbf{Uncertainty.} Figure~\ref{fig:uncertainty_by_season} and
Figure~\ref{fig:uncertainty_by_week} show that Rank seasons have the highest CV,
Percent seasons the lowest, and Bottom-2 seasons are intermediate. This aligns with
information content: Rank rules provide only ordering constraints, while Percent rules
impose numeric inequalities that significantly shrink the feasible solution space.


\subsection{Why This Modeling Choice is Advantageous}
This framework is \emph{rule-consistent} (mirrors official voting mechanics), \emph{robust} 
(regularization avoids unstable inverse solutions), and \emph{interpretable} (each constraint 
corresponds to a concrete production rule). It separates information available in a given week 
from future outcomes, preventing leakage while still exploiting placement in the only defensible way: 
ordering within the same elimination set.


% ============================================
% Full modeling for Question 1 (all seasons)
% ============================================

% \section{Task 1: Vote Estimation}

% \subsection{Why vote estimation is special and modeling choice}
% Vote estimation for Dancing with the Stars is an inverse problem: judges' scores and weekly elimination outcomes are observed, while the audience votes are unobserved. This differs from a standard prediction task because there is no direct ground-truth target to supervise a regression or classification model. Supervised learning (e.g. direct regression of votes from judges' scores and contestant features) cannot guarantee that the inferred votes reproduce the observed eliminations, and it does not quantify the non-uniqueness inherent to the inverse problem. For these reasons we avoid pure predictive algorithms and instead adopt a constrained inverse-estimation approach that enforces the show's elimination rules as hard or soft constraints.

% Our chosen framework is a convex-optimization-based inverse model (Percent-regime optimization) with discrete handling for Rank-type seasons and a probabilistic component for Bottom2 seasons. Brief justification:
% \begin{itemize}
% \item Rule constraints can be expressed as linear inequalities or permutation constraints, making them easy to embed directly into an optimization problem;
% \item For Percent-type seasons, the decision variables are vote shares; with appropriate regularization, the problem can be reformulated as a convex program, ensuring a global optimum;
% \item For Rank or Bottom2 seasons, integer or probabilistic methods are used to handle non-convexity, while maintaining consistency with a joint, season-wide optimization framework;
% \item Smoothing and maximum-entropy priors are introduced as weak priors, preserving interpretability while avoiding the artificial introduction of extreme solutions.
% \end{itemize}

% \subsection{Convex optimization model (overall)}

% \subsubsection{Problem Framing}
% The core challenge of Task 1 is an \emph{inverse problem}: fan vote totals are not publicly available, yet the observed eliminations place constraints on what the votes could have been. We seek weekly fan vote shares $f_{i,t}$ that are \emph{consistent} with the stated elimination rule while encoding minimal behavioral assumptions.

% \subsubsection{Data Interpretation and Notation}
% Let $J_{i,t}$ be contestant $i$'s total judges score in week $t$ (sum of available judges; missing values ignored). The data use $0$ as a placeholder after elimination, so the active set is
% \begin{equation}
% A_t=\{i: J_{i,t}>0\}.
% \end{equation}
% We treat the last positive score week as the primary elimination signal:
% \begin{equation}
% \mathrm{exit}(i)=\max\{t: J_{i,t}>0\}.
% \end{equation}
% The \texttt{results} text is used only to flag \texttt{Withdrew} cases, while finals ranking is taken from the native \texttt{placement} column (complete for all contestants).

% We estimate weekly fan vote \emph{shares}
% \begin{equation}
% f_{i,t}\ge 0,\qquad \sum_{i\in A_t} f_{i,t}=1,
% \end{equation}
% and define judge shares $j_{i,t}=J_{i,t}/\sum_{k\in A_t}J_{k,t}$. Let $E_t\subseteq A_t$ be the vote-determined eliminated set (excluding \textit{Withdrew}) and $S_t=A_t\setminus E_t$.

% \subsubsection{Season-dependent regimes and their mathematical constraints}
% The show uses three distinct regimes across seasons; each regime determines how eliminations constrain $f_{i,t}$:
% \begin{itemize}
% \item \textbf{Rank (Seasons 1--2)}: combined rank = judge rank + fan rank. Fan ranks must form a permutation $r^F_{\cdot,t}$; eliminated contestants have the worst combined ranks. This leads to integer/permutation constraints on ranks and an exponentiation mapping to convert ranks into shares:
% \begin{equation}
% f_{i,t}=\frac{\exp(-\gamma(r^F_{i,t}-1))}{\sum_{p\in A_t}\exp(-\gamma(r^F_{p,t}-1))}.
% \end{equation}
% \item \textbf{Percent (Seasons 3--27)}: combined score $c_{i,t}=j_{i,t}+f_{i,t}$. Eliminated contestants must lie among the lowest by combined score. The Percent regime admits a convex formulation via linear inequality constraints:
% \begin{equation}
% c_{e,t}\le c_{p,t}+\delta_t,\quad \forall e\in E_t,\ \forall p\in S_t,
% \label{eq:percent_hard}
% \end{equation}
% where $\delta_t\ge0$ is slack.
% \item \textbf{Bottom2 + Judges' choice (Seasons 28--34)}: bottom-two by combined rank are identified; judges choose one to eliminate. The bottom-two partner is unobserved, so we use a probabilistic judges-save model for partner selection (sigmoid in judge-score difference) and either enumerate feasible partners or sample in an ensemble.
% \end{itemize}

% \subsubsection{Optimization objective (Percent seasons)}
% For Percent seasons we use a two-stage strategy. Stage 1 minimizes slack $\delta$ to achieve maximum consistency. Stage 2 solves a convex regularized objective to break ties and enforce smoothness and high entropy:
% \begin{equation}
% \min_{f,\delta}\ \ M\sum_{t=1}^{T}\delta_t+\beta\sum_{t=2}^{T}\sum_{i\in A_t\cap A_{t-1}}(f_{i,t}-f_{i,t-1})^2+\alpha\sum_{t=1}^{T}\sum_{i\in A_t} f_{i,t}\log f_{i,t},
% \label{eq:percent_obj}
% \end{equation}
% subject to simplex constraints and Eq.~\eqref{eq:percent_hard}.

% The first term prioritizes consistency (minimal slack), the second enforces temporal smoothness, and the third encodes a maximum-entropy preference. Hyperparameters $M,\beta,\alpha$ are chosen by sensitivity analysis; $M$ is set large to prioritize feasibility in Stage 1.

% \subsection{Consistency Evaluation}
% \label{sec:task1_consistency}

% To assess how well the estimated fan votes reproduce the observed elimination outcomes, we define several quantitative metrics.

% \subsubsection{Constraint Satisfaction Rate (CSR)}

% The \textbf{Constraint Satisfaction Rate (CSR)} measures the fraction of weeks where the estimated votes perfectly satisfy the elimination rule:
% \begin{equation}
% \text{CSR}_s = \frac{1}{T_s} \sum_{t=1}^{T_s} \mathbb{I}[\text{elimination rule satisfied in week } t],
% \label{eq:csr}
% \end{equation}
% where $T_s$ is the number of weeks in season $s$, and $\mathbb{I}[\cdot]$ is the indicator function. For Percent seasons, the rule is satisfied if $\delta_t = 0$; for Rank/Bottom2 seasons, we check whether the estimated ranks/combined scores produce the observed elimination.

% \subsubsection{Jaccard Similarity}

% For multi-elimination weeks, we compute the \textbf{Jaccard similarity} between the observed eliminated set $E_t^{\text{obs}}$ and the predicted eliminated set $E_t^{\text{pred}}$:
% \begin{equation}
% J_t = \frac{|E_t^{\text{obs}} \cap E_t^{\text{pred}}|}{|E_t^{\text{obs}} \cup E_t^{\text{pred}}|}.
% \label{eq:jaccard}
% \end{equation}
% $J_t = 1$ indicates perfect agreement; $J_t < 1$ indicates partial mismatch.

% \subsubsection{Margin of Violation}

% For weeks with non-zero slack $\delta_t > 0$, the \textbf{margin of violation} quantifies the magnitude of inconsistency:
% \begin{equation}
% \Delta_t = \max_{e\in E_t, p\in S_t} \left[ c_{e,t} - c_{p,t} \right]_+,
% \label{eq:margin}
% \end{equation}
% where $[\cdot]_+ = \max(0, \cdot)$. Larger $\Delta_t$ indicates stronger violation of the elimination rule (i.e., eliminated contestants had higher combined scores than some safe contestants).

% \subsubsection{Slack Distribution}

% We analyze the distribution of slack variables $\{\delta_t\}$ across weeks and seasons:
% \begin{itemize}
%     \item \textbf{Zero-slack weeks:} Weeks where $\delta_t = 0$ (perfect consistency).
%     \item \textbf{Non-zero slack weeks:} Weeks where $\delta_t > 0$ (relaxed constraints needed).
%     \item \textbf{Mean slack:} $\bar{\delta}_s = \frac{1}{T_s} \sum_{t=1}^{T_s} \delta_t$.
% \end{itemize}

% \subsubsection{Aggregate Consistency by Regime}

% Table~\ref{tab:csr_detailed} provides detailed CSR statistics by regime:

% \begin{table}[H]
% \centering
% \caption{Detailed Constraint Satisfaction Rate by Regime}
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Regime} & \textbf{Seasons} & \textbf{Total Weeks} & \textbf{Perfect Weeks} & \textbf{CSR (\%)} & \textbf{Mean Slack} \\
% \midrule
% Rank & 1--2 & 18 & 18 & 100.0 & 0.000 \\
% Percent & 3--27 & 312 & 312 & 100.0 & 0.000 \\
% Bottom2 & 28--34 & 89 & 71 & 79.7 & 0.042 \\
% \bottomrule
% \end{tabular}
% \label{tab:csr_detailed}
% \end{table}

% \paragraph{Interpretation:}
% \begin{itemize}
%     \item \textbf{Percent regime (100\% CSR):} Hard constraints (Eq.~\eqref{eq:percent_hard}) ensure perfect consistency by construction. All eliminations are exactly reproduced.
%     \item \textbf{Rank regime (100\% CSR):} Permutation-based search over fan ranks finds consistent solutions for all weeks in Seasons 1--2.
%     \item \textbf{Bottom2 regime (79.7\% CSR):} The unobserved bottom-two partner introduces ambiguity. In $\sim20\%$ of weeks, no feasible partner exists that satisfies both the elimination and the judges' save probability, requiring slack relaxation.
% \end{itemize}

% \subsection{Uncertainty Measurement}
% \label{sec:task1_uncertainty}

% Because the inverse problem is inherently ill-posed (multiple vote distributions can produce the same eliminations), we quantify the uncertainty of estimated fan votes using an ensemble approach.

% \subsubsection{Ensemble Construction}

% We generate $K$ perturbed versions of the judges' scores:
% \begin{equation}
% J_{i,t}^{(k)} = J_{i,t} + \epsilon_{i,t}^{(k)}, \quad \epsilon_{i,t}^{(k)} \sim \mathcal{N}(0, \sigma^2),
% \label{eq:ensemble_perturbation}
% \end{equation}
% where $\sigma$ is the perturbation standard deviation (typically set to 5\% of the mean judge score). For each perturbed dataset, we re-run the optimization to obtain $f_{i,t}^{(k)}$.

% \subsubsection{Certainty Metric}

% For each contestant-week pair $(i,t)$, we compute the \textbf{certainty} as:
% \begin{equation}
% \text{Certainty}_{i,t} = 1 - \frac{\text{std}(f_{i,t}^{(1)}, \ldots, f_{i,t}^{(K)})}{\text{mean}(f_{i,t}^{(1)}, \ldots, f_{i,t}^{(K)})},
% \label{eq:certainty}
% \end{equation}
% i.e., one minus the coefficient of variation (CV). Higher certainty (closer to 1) indicates that the estimate is robust to perturbations; lower certainty indicates high sensitivity.

% \subsubsection{Confidence Intervals}

% We construct 95\% confidence intervals using the ensemble:
% \begin{equation}
% \text{CI}_{95\%}(f_{i,t}) = \left[ Q_{2.5\%}(f_{i,t}^{(1:K)}), \, Q_{97.5\%}(f_{i,t}^{(1:K)}) \right],
% \label{eq:ci}
% \end{equation}
% where $Q_p$ denotes the $p$-th percentile.

% \subsubsection{Variation by Contestant and Week Type}

% We analyze certainty variation along two dimensions:

% \paragraph{1. By Week Type}
% Certainty varies significantly by week type (detailed analysis in Section~\ref{sec:certainty}):
% \begin{itemize}
%     \item \textbf{Finals weeks:} Certainty $\approx 1.0$ (small active set, strong ranking constraint).
%     \item \textbf{Single elimination weeks:} High certainty (0.98 for Percent, 0.82 for Bottom2).
%     \item \textbf{Multi-elimination weeks:} Lower certainty (0.90--0.97) due to tighter ordering requirements.
%     \item \textbf{No-elimination weeks:} Slightly higher certainty (fewer constraints, but also less information).
% \end{itemize}

% \paragraph{2. By Contestant Popularity}
% We correlate certainty with estimated vote share:
% \begin{equation}
% \rho_{\text{cert-vote}} = \text{corr}(\text{Certainty}_{i,t}, f_{i,t}).
% \label{eq:corr_cert_vote}
% \end{equation}
% Preliminary analysis shows weak negative correlation ($\rho \approx -0.12$), suggesting that low-vote contestants (near elimination threshold) have slightly higher uncertainty.

% \paragraph{3. By Regime}
% Regime-level certainty (Section~\ref{sec:certainty}) shows:
% \begin{itemize}
%     \item \textbf{Percent regime:} Avg certainty = 0.982 (very high).
%     \item \textbf{Rank regime:} Avg certainty = 0.946 (high, limited by permutation ambiguity).
%     \item \textbf{Bottom2 regime:} Avg certainty = 0.786 (moderate, due to judges' save and partner uncertainty).
% \end{itemize}

% \subsubsection{Algorithm for Uncertainty Quantification}

% The complete ensemble-based uncertainty quantification procedure is summarized in Algorithm~\ref{alg:uncertainty}.

% \begin{algorithm}[H]
% \caption{Ensemble-based Uncertainty Quantification}
% \label{alg:uncertainty}
% \begin{algorithmic}[1]
% \State \textbf{Input:} Judges' scores $\{J_{i,t}\}$, elimination outcomes $\{E_t\}$, ensemble size $K$, perturbation std $\sigma$
% \State \textbf{Output:} Mean vote shares $\bar{f}_{i,t}$, certainty $\text{Certainty}_{i,t}$, confidence intervals $\text{CI}_{95\%}(f_{i,t})$
% \For{$k = 1$ to $K$}
%     \State Sample perturbations: $\epsilon_{i,t}^{(k)} \sim \mathcal{N}(0, \sigma^2)$ for all $i,t$
%     \State Compute perturbed scores: $J_{i,t}^{(k)} = J_{i,t} + \epsilon_{i,t}^{(k)}$
%     \State Solve optimization problem (Eq.~\eqref{eq:percent_obj}) with $J^{(k)}$ to obtain $f^{(k)}_{i,t}$
%     \State Store $f^{(k)}_{i,t}$ in ensemble array
% \EndFor
% \For{each contestant-week pair $(i,t)$}
%     \State Compute mean: $\bar{f}_{i,t} = \frac{1}{K}\sum_{k=1}^{K} f_{i,t}^{(k)}$
%     \State Compute std: $\sigma_{i,t} = \sqrt{\frac{1}{K-1}\sum_{k=1}^{K}(f_{i,t}^{(k)} - \bar{f}_{i,t})^2}$
%     \State Compute CV: $\text{CV}_{i,t} = \sigma_{i,t} / \bar{f}_{i,t}$
%     \State Compute certainty: $\text{Certainty}_{i,t} = 1 - \text{CV}_{i,t}$
%     \State Compute 95\% CI: $\text{CI}_{95\%}(f_{i,t}) = [Q_{2.5\%}, Q_{97.5\%}]$ from $\{f_{i,t}^{(k)}\}_{k=1}^K$
% \EndFor
% \State \Return $\bar{f}_{i,t}$, $\text{Certainty}_{i,t}$, $\text{CI}_{95\%}(f_{i,t})$
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Validation: Does Uncertainty Vary?}

% \textbf{Answer: Yes.} Our analysis (Table~\ref{tab:weektype_certainty}) demonstrates that:
% \begin{enumerate}
%     \item \textbf{Certainty varies by regime:} Percent (0.982) > Rank (0.946) > Bottom2 (0.786).
%     \item \textbf{Certainty varies by week type:} Finals (1.0) > single\_elim (0.82--0.99) > multi\_elim (0.90--0.97).
%     \item \textbf{Certainty varies by contestant:} Weak correlation with vote share ($\rho \approx -0.12$), but measurable variation exists.
% \end{enumerate}

% This confirms that the inverse problem's non-uniqueness is not uniform: some weeks and contestants are tightly constrained (high certainty), while others admit a range of plausible vote distributions (lower certainty).



\section{Model Results and Analysis}

\subsection{Consistency with Eliminations}
Table~\ref{tab:csr} shows the CSR by regime. Percent seasons achieve 100\% consistency by construction
of hard constraints. Bottom2 seasons remain below 100\% because the bottom-two partner is unobserved
and some weeks require relaxed constraints.

\begin{table}[H]
\centering
\caption{Constraint Satisfaction Rate by Regime}
\begin{tabular}{lcc}
\toprule
\textbf{Regime} & \textbf{Seasons} & \textbf{CSR} \\
\midrule
Rank & 1--2 & 100.0\% \\
Percent & 3--27 & 100.0\% \\
Bottom2 & 28--34 & 79.7\% \\
\bottomrule
\end{tabular}
\label{tab:csr}
\end{table}

\subsection{Certainty of Estimated Fan Votes}
\label{sec:certainty}
Average certainty by regime is summarized in Table~\ref{tab:certainty}. Percent seasons are most constrained,
while Bottom2 seasons are least certain because the judges' save introduces additional ambiguity.

\begin{table}[H]
\centering
\caption{Average Certainty by Regime (K=20 Ensemble)}
\begin{tabular}{lcc}
\toprule
\textbf{Regime} & \textbf{Avg Certainty} & \textbf{Interpretation} \\
\midrule
Rank & 0.946 & High (permutation ambiguity) \\
Percent & 0.982 & Very high (hard constraints) \\
Bottom2 & 0.786 & Moderate (judges' save + partner uncertainty) \\
\bottomrule
\end{tabular}
\label{tab:certainty}
\end{table}

\subsection{Certainty Variation by Week Type}
Certainty is not uniform across week types. Table~\ref{tab:weektype_certainty} reports the mean certainty
for contestant-week observations by week type.

\begin{table}[H]
\centering
\caption{Certainty by Week Type}
\begin{tabular}{lccc}
\toprule
\textbf{Regime} & \textbf{Week Type} & \textbf{Count} & \textbf{Avg Certainty} \\
\midrule
Percent & single\_elim & 1391 & 0.986 \\
Percent & no\_elim\_interp & 318 & 0.988 \\
Percent & multi\_elim\_2 & 200 & 0.969 \\
Percent & multi\_elim\_3 & 6 & 0.901 \\
Percent & finals & 82 & 1.000 \\
Bottom2 & single\_elim & 370 & 0.824 \\
Bottom2 & bottom2\_relaxed & 108 & 0.818 \\
Bottom2 & no\_elim\_interp & 108 & 0.826 \\
Bottom2 & multi\_elim\_2 & 85 & 0.851 \\
Bottom2 & finals & 31 & 1.000 \\
\bottomrule
\end{tabular}
\label{tab:weektype_certainty}
\end{table}

\paragraph{Interpretation.}
Multi-elimination weeks are the most uncertain due to tighter ordering requirements, while finals weeks are most
stable because the ranking constraint is strong and the active set is small.

\subsection{Why This Algorithm is Reasonable}
\begin{itemize}
\item \textbf{Inverse framing matches the data.} Fan votes are unobserved; eliminations provide indirect constraints.
\item \textbf{Rule-consistency is primary.} The model enforces the show's mechanism by construction (with slack when needed).
\item \textbf{Regularization encodes minimal behavioral assumptions.} Smoothness reflects persistent popularity; maximum entropy avoids inventing extreme vote spikes without evidence.
\item \textbf{Uncertainty is part of the answer.} Non-identifiability is handled by an ensemble that quantifies certainty per contestant/week.
\end{itemize}

\section{Strengths and Weaknesses}

\subsection{Strengths}
\begin{itemize}
\item \textbf{Rule-Faithful:} The model explicitly enforces the stated elimination mechanism for each season regime (Percent, Rank, Bottom2).
\item \textbf{Handles All Regimes:} A unified framework addresses the three distinct rule types across 34 seasons.
\item \textbf{Quantified Uncertainty:} Ensemble-based certainty measures provide per-contestant/week confidence, answering whether certainty varies (it does).
\item \textbf{Interpretable Diagnostics:} Slack and CSR serve as transparent indicators of ``surprise eliminations'' rather than model failures.
\end{itemize}

\subsection{Weaknesses}
\begin{itemize}
\item \textbf{Bottom2 Ambiguity:} The unobserved partner in Seasons 28--34 limits achievable CSR (79.7\%).
\item \textbf{Judges' Save Not Modeled:} We do not model judge preferences when choosing between bottom-two contestants.
\item \textbf{Hyperparameter Sensitivity:} Results depend on choices of $\alpha$, $\beta$, $\gamma$, though ensemble averaging mitigates this.
\end{itemize}

\begin{thebibliography}{99}
\bibitem{Boyd2004} Boyd, S., \& Vandenberghe, L. (2004). \emph{Convex optimization}. Cambridge University Press.

\bibitem{WikiDWTS} Wikipedia contributors. (2024). Dancing with the Stars (American TV series). \emph{Wikipedia, The Free Encyclopedia}. Retrieved January 2026.

\bibitem{Jaynes1957} Jaynes, E. T. (1957). Information theory and statistical mechanics. \emph{Physical Review}, 106(4), 620--630.

\bibitem{GoogleTrendsTV} Goel, S., Hofman, J. M., Lahaie, S., Pennock, D. M., \& Watts, D. J. (2010). Predicting consumer behavior with Web search. \emph{Proceedings of the National Academy of Sciences}, 107(41), 17486--17490.
\end{thebibliography}

\begin{appendices}


\end{appendices}


\AImatter

\begin{ReportAiUse}{1}
\bibitem{AI1}
GitHub Copilot (Agent Mode)\\
Query: Develop convex optimization model for DWTS fan vote estimation\\
Output: Used to assist with model formulation, code debugging, and LaTeX formatting for the inverse fan vote estimation problem.
\end{ReportAiUse}

\end{document}
