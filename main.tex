%%
%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%% !Mode:: "TeX:UTF-8"
%% -----------------------------------
%% This is a generated file.
%% 
%% Copyright (C) 2010 -- 2015 by latexstudio
%%       2014 -- 2019 by Liam Huang
%%       2019 -- present by latexstudio.net
%% 
%% License: The LaTeX Project Public License 1.3c
%% 
%% The Current Maintainer of this work is latexstudio.net.
%% 
\documentclass{mcmthesis}
 %\documentclass[CTeX = true]{mcmthesis}  % 当使用 CTeX 套装时请注释上一行使用该行的设置
\mcmsetup{tstyle=\color{red}\bfseries,%修改题号，队号的颜色和加粗显示，黑色可以修改为 black
        tcn = 2607256, problem = C, %修改队号，参赛题号
        sheet = true, titleinsheet = false, keywordsinsheet = true,%修改sheet显示信息
        titlepage = false, abstract = true}

  %四款字体可以选择
  %\usepackage{times}
  \usepackage{newtxtext,newtxmath} %CTeX 无此字体，可用 txfonts 替代，请使用新版 TeXLive.
  %\usepackage{palatino}
  %\usepackage{txfonts}

\usepackage{indentfirst}  %首行缩进，注释掉，首行就不再缩进。
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\title{The \LaTeX{} Template for MCM Version \MCMversion}
\author{\small \href{https://www.latexstudio.net/}
  {\includegraphics[width=7cm]{mcmthesis-logo}}}
\date{\today}
\begin{document}
\begin{abstract}
\par Use this template to begin typing the first page (summary page) of your electronic report. This
template uses a 12-point Times New Roman font. Submit your paper as an Adobe PDF
electronic file (e.g. 1111111.pdf), typed in English, with a readable font of at least 12-point type. 

Do not include the name of your school, advisor, or team members on this or any page. 

Be sure to change the control number and problem choice above. 

You may delete these instructions as you begin to type your report here.  

\textbf{Follow us @COMAPMath on Twitter or COMAPCHINAOFFICIAL on Weibo for the
most up to date contest information.}

\begin{keywords}
keyword1; keyword2
\end{keywords}
\end{abstract}
\maketitle
%% Generate the Table of Contents, if it's needed.
\tableofcontents
\newpage
%%
%% Generate the Memorandum, if it's needed.
%% \memoto{\LaTeX{}studio}
%% \memofrom{Liam Huang}
%% \memosubject{Happy \TeX{}ing!}
%% \memodate{\today}
%% \memologo{\LARGE I'm pretending to be a LOGO!}
%% \begin{memo}[Memorandum]
%%   \lipsum[1-3]
%% \end{memo}
%%
\section{Introduction}
\subsection{Background}

DWTS (Dancing with the Stars) is a popular television competition where celebrity contestants are paired with professional dancers to compete in weekly dance performances. Elimination and advancement results are determined by combining judges' scores and audience votes. The judges assess technical proficiency, which can be subjective, while fan votes are influenced by factors such as celebrity popularity and charisma. As a result, the competition's outcomes have often sparked discussions and controversies, despite the show's attempts to improve the integration of these two elements. In recent years, with increasing audience concerns about fairness in competitive variety shows, DWTS faces a critical need for a fair, impartial, and effective scoring system. This system must ensure both the program's entertainment value and the confidentiality of fan votes, securing a balance between professionalism and popularity. This is essential for DWTS to maintain its viewership and grow its brand in future seasons.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.35\linewidth]{figures/dwtc背景图.png}
%     \caption{Dancing with the Stars poster}
%     \label{fig:placeholder}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.37\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dwtc背景图.png}
    \end{minipage}
    \hspace{0.05\linewidth}
    \begin{minipage}{0.37\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dwtc问题综述图.png}
    \end{minipage}
    \textbf{\caption{Dancing with the Stars}}
    \label{fig:dwtc_overview}
\end{figure}



% \subsection{Restatement of the Problem}

% Given the background information and available data, this study addresses the following tasks.

% \textbf{Task 1: Fan Vote Estimation.}
% \textbf{1.1}\quad Construct a model to estimate weekly fan votes using judges’ scores, elimination outcomes, and contestant data.  
% \textbf{1.2}\quad Evaluate the consistency of the estimates by testing whether they reproduce the observed weekly eliminations.  
% \textbf{1.3}\quad Quantify the uncertainty of the estimated fan votes and examine whether it varies across contestants or weeks.

% \textbf{Task 2: Comparison of Vote Combination and Elimination Rules.}
% \textbf{2.1}\quad Compare the rank-based and percentage-based vote combination methods across seasons and analyze potential biases.  
% \textbf{2.2}\quad Evaluate the impact of alternative elimination procedures, including judges selecting from the bottom two contestants.  
% \textbf{2.3}\quad Recommend an appropriate combination and elimination approach for future seasons with justification.

% \textbf{Task 3: Impact of Contestant and Partner Characteristics.}
% \textbf{3.1}\quad Assess the influence of professional dancer traits and celebrity characteristics on overall performance and final results.  
% \textbf{3.2}\quad Compare the effects of these factors on judges’ scores versus fan votes.

% \textbf{Task 4: Alternative Scoring System.}
% \textbf{4.1}\quad Design an alternative method for combining judges’ scores and fan votes.  
% \textbf{4.2}\quad Demonstrate that the proposed system is more fair or otherwise improves the competition.
\subsection{Restatement of the Problem}

Given the background information and available data, this study addresses the following tasks.

\textbf{Task 1: Fan Vote Estimation.}\quad
A mathematical model is developed to estimate weekly fan vote totals for each contestant using judges’ scores, elimination outcomes, and contestant data. The model’s consistency is evaluated by its ability to reproduce observed weekly eliminations, and the uncertainty of the estimated fan votes is quantified and analyzed across contestants and weeks.

\textbf{Task 2: Comparison of Vote Combination and Elimination Rules.}\quad
Using the estimated fan votes, the rank-based and percentage-based methods for combining judges’ scores and fan votes are compared across seasons. The effects of alternative elimination procedures, including judges selecting from the bottom two contestants, are also evaluated, leading to a justified recommendation for future seasons.

\textbf{Task 3: Impact of Contestant and Partner Characteristics.}\quad
The impacts of professional dancer traits and celebrity characteristics on competition outcomes are analyzed, with a comparison of their influences on judges’ scores and fan votes.

\textbf{Task 4: Alternative Scoring System.}
An alternative scoring system combining \quad
judges’ scores and fan votes is proposed and evaluated to determine whether it improves fairness or other aspects of the competition.


\section{Preparations for Modeling}

\subsection{Model Assumptions}

To construct a mathematically rigorous inverse estimation model, we adopt the following assumptions, categorized by their nature and justification.

\subsubsection{Fan Vote Behavior Assumptions}

\textbf{Assumption 1 (Vote Share Sufficiency):} \label{ass:vote_share}
Fan votes are modeled as \emph{shares} (proportions) rather than absolute vote counts:
\begin{equation}
f_{i,t}\ge 0,\qquad \sum_{i\in A_t} f_{i,t}=1.
\end{equation}
\emph{Justification:} The elimination rule depends only on relative vote magnitudes, not total vote volume. This reduces the dimensionality of the inverse problem by eliminating the non-identifiable parameter $T_t$ (total votes cast in week $t$). Absolute vote counts are computed post hoc for presentation purposes only: $V_{i,t}=T_t\cdot f_{i,t}$ with $T_t=10^7$ (arbitrary scale).

\textbf{Assumption 2 (Temporal Continuity of Popularity):}
Contestant popularity, as reflected in fan vote shares, evolves smoothly across consecutive weeks:
\begin{equation}
\text{Minimize}\quad \sum_{t=2}^{T}\sum_{i\in A_t\cap A_{t-1}} (f_{i,t}-f_{i,t-1})^2.
\end{equation}
\emph{Justification:} In reality, a contestant's fan base does not fluctuate wildly from week to week unless extraordinary events occur (e.g., viral performances, scandals). This regularization prevents the model from overfitting to single-week eliminations by inventing implausible vote spikes.

\textbf{Assumption 3 (Maximum Entropy Principle):}
In the absence of evidence to the contrary, fan vote distributions should be as uniform as possible (maximum entropy):
\begin{equation}
\text{Minimize}\quad \sum_{t=1}^{T}\sum_{i\in A_t} f_{i,t}\log f_{i,t}.
\end{equation}
\emph{Justification:} This is a standard principle in inverse problems \cite{Jaynes1957}. By maximizing entropy (equivalently, minimizing negative entropy), we avoid introducing spurious structure (e.g., extreme vote concentration) that is not forced by the elimination constraints. The solution remains as ``agnostic'' as the data allow.

\subsubsection{Rule Mechanism Assumptions}

\textbf{Assumption 4 (Rule Faithfulness):}
The show strictly adheres to its publicly stated elimination rules in each season:
\begin{itemize}
    \item \textbf{Seasons 1--2 (Rank):} Eliminated contestants have the worst combined rank (judge rank + fan rank).
    \item \textbf{Seasons 3--27 (Percent):} Eliminated contestants have the lowest combined score (judge share + fan share).
    \item \textbf{Seasons 28--34 (Bottom2):} The bottom two by combined rank are identified; judges eliminate one of them.
\end{itemize}
\emph{Justification:} Publicly documented rule changes align with our season classifications \cite{WikiDWTS}. Any ``upset'' eliminations (where constraints cannot be perfectly satisfied) are handled via slack variables, not by discarding the rule assumption.

\textbf{Assumption 5 (Elimination Consistency):}
For Percent seasons, eliminated contestants $e\in E_t$ must have combined scores no greater than surviving contestants $p\in S_t$:
\begin{equation}
c_{e,t}\le c_{p,t}+\delta_t,\qquad \forall e\in E_t,\ \forall p\in S_t,
\end{equation}
where $\delta_t\ge 0$ is a slack variable to absorb ties or anomalies.
\emph{Justification:} This is the mathematical translation of the ``lowest combined score is eliminated'' rule. Slack is introduced only when necessary (e.g., multi-elimination weeks with tight orderings).

\textbf{Assumption 6 (Judges' Save Preference):} \label{ass:judges_save}
When judges select from the bottom two contestants $\{e, b\}$ (Seasons 28--34), they probabilistically favor saving the contestant with higher judge scores:
\begin{equation}
\Pr(\text{eliminate } e \mid e, b) = \sigma\left(\kappa(J_{b,t} - J_{e,t})\right),
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function and $\kappa>0$ controls sensitivity.
\emph{Justification:} Judges are experts evaluating technical merit; their save decision should correlate with their own scores. The probabilistic formulation captures uncertainty in unobserved bottom-two partners.

\subsubsection{External Validation Assumptions}

\textbf{Assumption 7 (Google Trends as Attention Proxy):} \label{ass:google_trends}
Public attention to contestants, as measured by Google search trends during competition weeks, serves as an independent validation signal for estimated fan vote shares.
\emph{Justification:} Prior studies on televised competitions have shown strong correlation between online search activity and voting behavior \cite{GoogleTrendsTV}. While not used as a hard constraint in the base model, Google Trends data can:
\begin{itemize}
    \item Cross-validate high-popularity contestants (e.g., if estimated $f_{i,t}$ is low but Google Trends spike, the model may be under-predicting fan votes).
    \item Identify ``dark horse'' contestants (low pre-show visibility but strong fan vote performance).
    \item Provide Bayesian priors in extended models: $f_{i,t}^{\text{prior}}\propto \text{GoogleTrends}_{i,t}$.
\end{itemize}

\subsubsection{Uncertainty Quantification Assumptions}

\textbf{Assumption 8 (Perturbation Continuity):}
Small perturbations in judge scores ($J_{i,t}\to J_{i,t}+\epsilon_{i,t}$) induce bounded changes in estimated fan votes, allowing ensemble-based uncertainty quantification.
\emph{Justification:} If the inverse problem were extremely ill-posed, tiny noise would cause unbounded solution variation. Our ensemble experiments (Section~\ref{sec:certainty}) confirm finite sensitivity, validating this assumption.

\textbf{Assumption 9 (Independence of Perturbations):}
Judge score errors $\epsilon_{i,t}$ are independent across contestants and weeks.
\emph{Justification:} This simplifies ensemble construction. In reality, judges may exhibit systematic biases, but modeling such correlations is beyond the scope of this inverse problem framework.

\subsubsection{Optimization Assumptions}

\textbf{Assumption 10 (Convexity for Percent Seasons):}
The elimination constraints in Percent seasons (Eq.~\ref{eq:percent_hard}) define a convex feasible region when combined with simplex constraints.
\emph{Justification:} Convexity guarantees global optimality of the solution and computational tractability via standard solvers (e.g., CVXPY \cite{Boyd2004}).

\textbf{Assumption 11 (Hierarchical Objective Prioritization):}
Consistency with eliminations takes absolute priority over smoothness and entropy:
\begin{enumerate}
    \item \textbf{Stage 1:} Minimize slack $\sum_t \delta_t$ to achieve maximum consistency.
    \item \textbf{Stage 2:} Subject to $\delta_t\le \delta_t^\star+\varepsilon$, optimize smoothness and entropy.
\end{enumerate}
\emph{Justification:} The primary goal is to \emph{explain} observed eliminations. Behavioral regularization (smoothness, entropy) is secondary and serves only as a tie-breaker when multiple solutions achieve equal consistency.

\subsection{Notations}

\subsection{Data Processing}


% ============================================
% Full modeling for Question 1 (all seasons)
% ============================================

\section{Model: Inverse Estimation of Fan Votes}

\subsection{Problem Framing}
The core challenge of Question 1 is an \emph{inverse problem}: fan vote totals are not publicly available, 
yet the observed eliminations place constraints on what the votes could have been. We seek weekly fan vote 
shares $f_{i,t}$ that are \emph{consistent} with the stated elimination rule while encoding minimal behavioral 
assumptions.

\subsection{Data Preparation and Notation}
Let $J_{i,t}$ be contestant $i$'s total judges score in week $t$ (sum of available judges; missing values ignored).
The data use $0$ as a placeholder after elimination, so the active set is
\begin{equation}
A_t=\{i: J_{i,t}>0\}.
\end{equation}
We treat the last positive score week as the primary elimination signal:
\begin{equation}
\mathrm{exit}(i)=\max\{t: J_{i,t}>0\}.
\end{equation}
The \texttt{results} text is used only to flag \texttt{Withdrew} cases, while finals ranking is taken from the native
\texttt{placement} column (complete for all contestants).

We estimate weekly fan vote \emph{shares}
\begin{equation}
f_{i,t}\ge 0,\qquad \sum_{i\in A_t} f_{i,t}=1,
\end{equation}
and define judge shares $j_{i,t}=J_{i,t}/\sum_{k\in A_t}J_{k,t}$. Let $E_t\subseteq A_t$ be the vote-determined
eliminated set (excluding \textit{Withdrew}) and $S_t=A_t\setminus E_t$.

\subsection{Season-Dependent Rule Regimes}
The competition uses three different rules across its history:
\begin{itemize}
\item \textbf{S1--2 (Rank)}: combined rank = judge rank + fan rank; the worst sum is eliminated.
\item \textbf{S3--27 (Percent)}: combined score = judge share + fan share; the smallest sum is eliminated.
\item \textbf{S28--34 (Bottom2 + judges' choice)}: bottom two by combined rank; judges choose whom to eliminate.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/rules_chart.png}
    \caption{The Rules'Change}
    \label{fig:placeholder}
\end{figure}
\end{itemize}

\subsection{Regime A: Percent Seasons (S3--27)}
\paragraph{Goal.} Infer $f_{i,t}$ so that eliminated contestants are in the bottom-$|E_t|$ by combined score
$c_{i,t}=j_{i,t}+f_{i,t}$.

\paragraph{Multi-elimination constraint (with slack).} For each elimination week,
\begin{equation}
c_{e,t}\le c_{p,t}+\delta_t,\qquad \forall e\in E_t,\ \forall p\in S_t,
\label{eq:percent_hard}
\end{equation}
with slack $\delta_t\ge 0$ to absorb ties and special episodes. If $E_t=\varnothing$, no ordering constraint is imposed.

\paragraph{Convex optimization.} We select the ``least-assumptive'' explanation:
\begin{equation}
\min_{f,\delta}\ \ 
M\sum_{t=1}^{T}\delta_t
+\beta\sum_{t=2}^{T}\sum_{i\in A_t\cap A_{t-1}} (f_{i,t}-f_{i,t-1})^2
+\alpha\sum_{t=1}^{T}\sum_{i\in A_t} f_{i,t}\log f_{i,t},
\label{eq:percent_obj}
\end{equation}
subject to the simplex constraints and Eq.~\eqref{eq:percent_hard}.
The first term prioritizes consistency (minimal slack), the second encodes popularity persistence (temporal smoothness),
and the third is negative entropy (preferring high-entropy solutions unless forced otherwise).

\paragraph{Two-stage solve.} Consistency is prioritized via:
\begin{enumerate}
\item \textbf{Stage 1 (feasibility):} Minimize $\delta_t$ subject to constraints.
\item \textbf{Stage 2 (tie-break):} Restrict $\delta_t\le \delta_t^\star+\varepsilon$ and minimize
$\beta\,\mathcal{L}(\bm f_t)-\alpha\sum_{i}f_{i,t}\log f_{i,t}$,
where $\mathcal{L}$ is a softmax negative log-likelihood for multi-elimination weeks.
\end{enumerate}

\paragraph{Finals and no-elimination weeks.} For the final week, we enforce the ranking order
$c_{\pi(1),T}>c_{\pi(2),T}>\cdots$ using \textit{placement}. For no-elimination weeks,
we interpolate $f_{i,t}$ from adjacent weeks with the same active set.

\subsection{Regime B: Rank Seasons (S1--2)}
Let $r^J_{i,t}$ be the judge rank (1=best). Fan ranks $r^F_{i,t}$ must form a \emph{permutation} of
$\{1,\ldots,|A_t|\}$. The combined rank is
\begin{equation}
c_{i,t}=r^J_{i,t}+r^F_{i,t},
\end{equation}
and eliminated contestants are those with the largest $c_{i,t}$.
We construct a feasible permutation by assigning the worst fan ranks to eliminated contestants and
the best fan ranks to contestants with weak judge ranks, then perform swaps to satisfy the elimination constraint.
If no permutation makes all eliminated contestants the worst, we label the week as an \emph{upset}.
Fan ranks are mapped to vote shares by
\begin{equation}
f_{i,t}=\frac{\exp(-\gamma(r^F_{i,t}-1))}{\sum_{p\in A_t}\exp(-\gamma(r^F_{p,t}-1))}.
\end{equation}

\subsection{Regime C: Bottom2 + Judges' Save (S28--34)}
The bottom two are the contestants with the two highest combined ranks.
In single-elimination weeks, the eliminated contestant $e$ is known, but the bottom-two partner $b$ is unobserved.

\paragraph{Probabilistic Judges' Save Model.}
We model the judges' decision probabi listically. Given bottom-two contestants $\{e, b\}$, the probability
that judges eliminate $e$ (and save $b$) is:
\begin{equation}
\Pr(\text{elim } e \mid e, b) = \sigma\left(\kappa(J_{b,t} - J_{e,t})\right),
\label{eq:judges_save}
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function and $\kappa > 0$ controls sensitivity to score differences.
This encodes the intuition that judges prefer to save the higher-scoring contestant.

\paragraph{Partner Selection.}
For each candidate partner $b \in A_t \setminus \{e\}$:
if  fan ranks can be assigned and $\{e, b\}$ form the bottom two , compute $\Pr(\text{elim } e \mid e, b)$ via Eq.~\eqref{eq:judges_save}. And then select the partner with highest elimination probability (deterministic mode) or sample from
the normalized distribution (stochastic mode for ensemble).


\paragraph{Ensemble Sampling.}
In ensemble runs, we sample the partner from the probability distribution, capturing uncertainty
from both score perturbation \emph{and} judges' decision variability.
Multi-elimination weeks fall back to the rank-permutation construction.

\subsection{Consistency Measures}
We compute the following metrics:
\begin{itemize}
\item \textbf{Exact-Set Accuracy (CSR):} Fraction of elimination weeks whose inferred votes reproduce the observed elimination.
\item \textbf{Jaccard Similarity:} $\text{Jaccard}(t)=|\hat E_t\cap E_t|/|\hat E_t\cup E_t|$ for multi-elimination weeks.
\item \textbf{Decisiveness Margin:} Gap between the $|E_t|$-th and $(|E_t|+1)$-th combined scores.
\item \textbf{Slack:} For Percent seasons, the mean slack $\overline{\delta}$ summarizes how often constraints must be relaxed.
\end{itemize}

\subsection{Certainty Measures}
We quantify certainty via a perturb-and-resolve ensemble ($K=20$ runs):
\begin{itemize}
\item Perturb only active judge scores ($J_{i,t}>0$) with small noise.
\item For Bottom2 seasons: additionally sample the bottom-two partner from the probability distribution
defined by Eq.~\eqref{eq:judges_save}, capturing judges' decision uncertainty.
\item Re-solve the inverse problem.
\item Compute per-contestant/week statistics.
\end{itemize}
The certainty index is:
\begin{equation}
\text{Certainty}(i,t)=\frac{1}{1+\text{CV}(i,t)}=\frac{\mu_{i,t}}{\mu_{i,t}+\sigma_{i,t}},
\end{equation}
where $\mu_{i,t}$ and $\sigma_{i,t}$ are the ensemble mean and standard deviation of $f_{i,t}$.

\begin{algorithm}[H]
\caption{Inverse Estimation Workflow (All Seasons)}
\begin{algorithmic}[1]
\State \textbf{Input:} Judge scores $J_{i,t}$, results text, placements.
\State \textbf{Output:} Fan vote shares $\hat f_{i,t}$, CSR, certainty statistics.
\State Build $A_t=\{i:J_{i,t}>0\}$ and classify weeks by $E_t$.
\For{each season}
    \If{Percent (S3--27)}
        \State Stage 1: minimize slack $\delta_t$ under Eq.~\eqref{eq:percent_hard}.
        \State Stage 2: enforce $\delta_t\le \delta_t^\star+\varepsilon$ and minimize objective~\eqref{eq:percent_obj}.
    \ElsIf{Rank (S1--2)}
        \State Construct a valid fan-rank permutation; swap ranks until eliminations are worst.
    \ElsIf{Bottom2 (S28--34)}
        \State Enumerate feasible partners $b$; compute $\Pr(\text{elim } e \mid e,b)$ via Eq.~\eqref{eq:judges_save}.
        \State Select partner with highest probability (or sample in ensemble mode).
    \EndIf
    \State Enforce finals ordering from \texttt{placement}; interpolate $f_{i,t}$ for no-elimination weeks.
    \State Ensemble: perturb $J_{i,t}$, sample judges' save for Bottom2, re-solve, compute $\mu,\sigma,\text{Certainty}$.
    \State Validate eliminations and compute CSR.
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Model Results and Analysis}

\subsection{Consistency with Eliminations}
Table~\ref{tab:csr} shows the CSR by regime. Percent seasons achieve 100\% consistency by construction
of hard constraints. Bottom2 seasons remain below 100\% because the bottom-two partner is unobserved
and some weeks require relaxed constraints.

\begin{table}[H]
\centering
\caption{Constraint Satisfaction Rate by Regime}
\begin{tabular}{lcc}
\toprule
\textbf{Regime} & \textbf{Seasons} & \textbf{CSR} \\
\midrule
Rank & 1--2 & 100.0\% \\
Percent & 3--27 & 100.0\% \\
Bottom2 & 28--34 & 79.7\% \\
\bottomrule
\end{tabular}
\label{tab:csr}
\end{table}

\subsection{Certainty of Estimated Fan Votes}
\label{sec:certainty}
Average certainty by regime is summarized in Table~\ref{tab:certainty}. Percent seasons are most constrained,
while Bottom2 seasons are least certain because the judges' save introduces additional ambiguity.

\begin{table}[H]
\centering
\caption{Average Certainty by Regime (K=20 Ensemble)}
\begin{tabular}{lcc}
\toprule
\textbf{Regime} & \textbf{Avg Certainty} & \textbf{Interpretation} \\
\midrule
Rank & 0.946 & High (permutation ambiguity) \\
Percent & 0.982 & Very high (hard constraints) \\
Bottom2 & 0.786 & Moderate (judges' save + partner uncertainty) \\
\bottomrule
\end{tabular}
\label{tab:certainty}
\end{table}

\subsection{Certainty Variation by Week Type}
Certainty is not uniform across week types. Table~\ref{tab:weektype_certainty} reports the mean certainty
for contestant-week observations by week type.

\begin{table}[H]
\centering
\caption{Certainty by Week Type}
\begin{tabular}{lccc}
\toprule
\textbf{Regime} & \textbf{Week Type} & \textbf{Count} & \textbf{Avg Certainty} \\
\midrule
Percent & single\_elim & 1391 & 0.986 \\
Percent & no\_elim\_interp & 318 & 0.988 \\
Percent & multi\_elim\_2 & 200 & 0.969 \\
Percent & multi\_elim\_3 & 6 & 0.901 \\
Percent & finals & 82 & 1.000 \\
Bottom2 & single\_elim & 370 & 0.824 \\
Bottom2 & bottom2\_relaxed & 108 & 0.818 \\
Bottom2 & no\_elim\_interp & 108 & 0.826 \\
Bottom2 & multi\_elim\_2 & 85 & 0.851 \\
Bottom2 & finals & 31 & 1.000 \\
\bottomrule
\end{tabular}
\label{tab:weektype_certainty}
\end{table}

\paragraph{Interpretation.}
Multi-elimination weeks are the most uncertain due to tighter ordering requirements, while finals weeks are most
stable because the ranking constraint is strong and the active set is small.

\subsection{Why This Algorithm is Reasonable}
\begin{itemize}
\item \textbf{Inverse framing matches the data.} Fan votes are unobserved; eliminations provide indirect constraints.
\item \textbf{Rule-consistency is primary.} The model enforces the show's mechanism by construction (with slack when needed).
\item \textbf{Regularization encodes minimal behavioral assumptions.} Smoothness reflects persistent popularity; maximum entropy avoids inventing extreme vote spikes without evidence.
\item \textbf{Uncertainty is part of the answer.} Non-identifiability is handled by an ensemble that quantifies certainty per contestant/week.
\end{itemize}

\section{Strengths and Weaknesses}

\subsection{Strengths}
\begin{itemize}
\item \textbf{Rule-Faithful:} The model explicitly enforces the stated elimination mechanism for each season regime (Percent, Rank, Bottom2).
\item \textbf{Handles All Regimes:} A unified framework addresses the three distinct rule types across 34 seasons.
\item \textbf{Quantified Uncertainty:} Ensemble-based certainty measures provide per-contestant/week confidence, answering whether certainty varies (it does).
\item \textbf{Interpretable Diagnostics:} Slack and CSR serve as transparent indicators of ``surprise eliminations'' rather than model failures.
\end{itemize}

\subsection{Weaknesses}
\begin{itemize}
\item \textbf{Bottom2 Ambiguity:} The unobserved partner in Seasons 28--34 limits achievable CSR (79.7\%).
\item \textbf{Judges' Save Not Modeled:} We do not model judge preferences when choosing between bottom-two contestants.
\item \textbf{Hyperparameter Sensitivity:} Results depend on choices of $\alpha$, $\beta$, $\gamma$, though ensemble averaging mitigates this.
\end{itemize}

\begin{thebibliography}{99}
\bibitem{Boyd2004} Boyd, S., \& Vandenberghe, L. (2004). \emph{Convex optimization}. Cambridge University Press.

\bibitem{WikiDWTS} Wikipedia contributors. (2024). Dancing with the Stars (American TV series). \emph{Wikipedia, The Free Encyclopedia}. Retrieved January 2026.

\bibitem{Jaynes1957} Jaynes, E. T. (1957). Information theory and statistical mechanics. \emph{Physical Review}, 106(4), 620--630.

\bibitem{GoogleTrendsTV} Goel, S., Hofman, J. M., Lahaie, S., Pennock, D. M., \& Watts, D. J. (2010). Predicting consumer behavior with Web search. \emph{Proceedings of the National Academy of Sciences}, 107(41), 17486--17490.
\end{thebibliography}

\begin{appendices}

\section{Code Listing}
The fan vote estimation is implemented in Python. The complete code is provided below.

\lstinputlisting[language=Python, caption={Fan Vote Estimation Code}]{./code/fan_vote_convex.py}

\end{appendices}


\AImatter

\begin{ReportAiUse}{1}
\bibitem{AI1}
GitHub Copilot (Agent Mode)\\
Query: Develop convex optimization model for DWTS fan vote estimation\\
Output: Used to assist with model formulation, code debugging, and LaTeX formatting for the inverse fan vote estimation problem.
\end{ReportAiUse}

\end{document}
