%%
%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%% !Mode:: "TeX:UTF-8"
%% -----------------------------------
%% This is a generated file.
%% 
%% Copyright (C) 2010 -- 2015 by latexstudio
%%       2014 -- 2019 by Liam Huang
%%       2019 -- present by latexstudio.net
%% 
%% License: The LaTeX Project Public License 1.3c
%% 
%% The Current Maintainer of this work is latexstudio.net.
%% 
\documentclass{mcmthesis}
 %\documentclass[CTeX = true]{mcmthesis}  % 当使用 CTeX 套装时请注释上一行使用该行的设置
\mcmsetup{tstyle=\color{red}\bfseries,%修改题号，队号的颜色和加粗显示，黑色可以修改为 black
        tcn = 2607256, problem = C, %修改队号，参赛题号
        sheet = true, titleinsheet = false, keywordsinsheet = true,%修改sheet显示信息
        titlepage = false, abstract = true}

  %四款字体可以选择
  %\usepackage{times}
  \usepackage{newtxtext,newtxmath} %CTeX 无此字体，可用 txfonts 替代，请使用新版 TeXLive.
  %\usepackage{palatino}
  %\usepackage{txfonts}

\usepackage{indentfirst}  %首行缩进，注释掉，首行就不再缩进。
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\title{The \LaTeX{} Template for MCM Version \MCMversion}
\author{\small \href{https://www.latexstudio.net/}
  {\includegraphics[width=7cm]{mcmthesis-logo}}}
\date{\today}
\begin{document}
\begin{abstract}
\par Use this template to begin typing the first page (summary page) of your electronic report. This
template uses a 12-point Times New Roman font. Submit your paper as an Adobe PDF
electronic file (e.g. 1111111.pdf), typed in English, with a readable font of at least 12-point type. 

Do not include the name of your school, advisor, or team members on this or any page. 

Be sure to change the control number and problem choice above. 

You may delete these instructions as you begin to type your report here.  

\textbf{Follow us @COMAPMath on Twitter or COMAPCHINAOFFICIAL on Weibo for the
most up to date contest information.}

\begin{keywords}
keyword1; keyword2
\end{keywords}
\end{abstract}
\maketitle
%% Generate the Table of Contents, if it's needed.
\tableofcontents
\newpage
%%
%% Generate the Memorandum, if it's needed.
%% \memoto{\LaTeX{}studio}
%% \memofrom{Liam Huang}
%% \memosubject{Happy \TeX{}ing!}
%% \memodate{\today}
%% \memologo{\LARGE I'm pretending to be a LOGO!}
%% \begin{memo}[Memorandum]
%%   \lipsum[1-3]
%% \end{memo}
%%
\section{Introduction}
\subsection{Background}
\LaTeX\ is a document preparation system which uses the \TeX\
typesetting program. It enables you to produce
publication-quality documents with great accuracy and
consistency. \LaTeX\ works on any computer and produces
industry-standard PDF. It is available both in free (open-source)
and commercial implementations. \LaTeX\ can be used for any kind
of document, but it is especially suited to those with complex
structures, repetitive formatting, or notations like
mathematics. Install the software from
\url{www.tug.org/texlive/}.
\subsection{Restatement of the Problem}

Follow our WeChat official account for more LaTeX materials and information.

\centerline{\includegraphics[width=5cm]{qrcodewechat}}

\subsection{Syntax (how to type \LaTeX\ commands --- these
  are the rules)}

\lipsum[3]
\begin{itemize}
\item the angular velocity of the bat,
\item the velocity of the ball, and
\item the position of impact along the bat.
\end{itemize}
\lipsum[4]
\emph{center of percussion} [Brody 1986], \lipsum[5]

\begin{Theorem} \label{thm:latex}
\LaTeX
\end{Theorem}
\begin{Lemma} \label{thm:tex}
\TeX .
\end{Lemma}
\begin{proof}
The proof of theorem.
\end{proof}

\subsection{Other Assumptions}
\lipsum[6]
\begin{itemize}
\item
\item
\item
\item
\end{itemize}

\lipsum[7]

\section{Analysis of the Problem}
\begin{figure}[h]
\small
\centering
\includegraphics[width=8cm]{example-image-a}
\caption{The name of figure} \label{fig:aa}
\end{figure}

\lipsum[8] \eqref{aa}
\begin{equation}
a^2 \label{aa}
\end{equation}

\[
  \begin{pmatrix}{*{20}c}
  {a_{11} } & {a_{12} } & {a_{13} }  \\
  {a_{21} } & {a_{22} } & {a_{23} }  \\
  {a_{31} } & {a_{32} } & {a_{33} }  \\
  \end{pmatrix}
  = \frac{{Opposite}}{{Hypotenuse}}\cos ^{ - 1} \theta \arcsin \theta
\]
\lipsum[9]

\[
  p_{j}=\begin{cases} 0,&\text{if $j$ is odd}\\
  r!\,(-1)^{j/2},&\text{if $j$ is even}
  \end{cases}
\]

\lipsum[10]

\[
  \arcsin \theta  =
  \mathop{{\int\!\!\!\!\!\int\!\!\!\!\!\int}} \limits_\varphi
  {\mathop {\lim }\limits_{x \to \infty } \frac{{n!}}{{r!\left( {n - r}
  \right)!}}} \eqno (1)
\]
% ============================================
% Full modeling for Question 1 (all seasons)
% ============================================

\section{Model 1: Bayesian Inverse Estimation of Fan Votes}

\subsection{Problem Formulation: An Inverse Problem}
Estimating fan votes from observed eliminations is an \textbf{inverse problem}: given the output (who was eliminated), we infer the hidden input (fan votes). This problem is inherently \emph{under-determined}---many vote distributions can produce the same elimination outcome. Our approach addresses this by:
\begin{enumerate}
\item Formulating season-specific elimination rules as probabilistic constraints;
\item Incorporating temporal smoothness and maximum-entropy priors to regularize the solution space;
\item Quantifying uncertainty through Bayesian posterior inference.
\end{enumerate}

\subsection{Season Rules (Three Regimes)}
The show combines judges' scores and fan votes using different rules across seasons:
\begin{itemize}
\item \textbf{Seasons 1--2 (Rank)}: combine by ranks (judge rank + fan rank); the worst sum is eliminated.
\item \textbf{Seasons 3--27 (Percent)}: combine by shares (judge score share + fan vote share); the lowest sum is eliminated.
\item \textbf{Seasons 28--34 (Rank + Bottom2 + Judges' Save)}: identify bottom two by combined ranks, then judges choose one of them to eliminate.
\end{itemize}
Our goal is not to predict eliminations directly, but to \emph{infer fan votes} that make the observed eliminations consistent with the rule in that season.

\subsection{Weekly Data Construction}
Let $J_{i,t}$ be contestant $i$'s total judges score in week $t$ (sum over available judges; missing values ignored).
We define the active set in week $t$:
\begin{equation}
A_t=\{i: J_{i,t}>0\},
\end{equation}
since the dataset records $0$ after a contestant exits. Let $T$ be the last week of the season:
\begin{equation}
T=\max\{t:\exists i,\ J_{i,t}>0\}.
\end{equation}
We determine each contestant's last active week
\begin{equation}
\mathrm{exit}(i)=\max\{t\le T: J_{i,t}>0\}.
\end{equation}
We classify each contestant's exit into three categories based on the \texttt{results} field:

\paragraph{Case 1: Vote-determined elimination (normal case).}
If $\texttt{results}=\texttt{"Eliminated Week }t\texttt{"}$, contestant $i$ was eliminated by vote at week $t$.
This provides a \textbf{hard constraint}: $i$ must have the lowest (Percent) or highest (Rank) combined score among $A_t$.

\paragraph{Case 2: Withdrew (exogenous attrition).}
If $\texttt{results}=\texttt{"Withdrew"}$, the contestant left for non-voting reasons (injury, personal, etc.).
\begin{itemize}
\item Withdrew contestants contribute to $A_t$ in weeks before exit.
\item They are \textbf{excluded from elimination constraints}: no vote inference is derived from their exit.
\item Their vote shares are still estimated (regularized toward uniform) but with \textbf{high uncertainty}.
\end{itemize}

\paragraph{Case 3: Finals ranking (last week).}
In the final week $T$, no elimination occurs. Instead, the top 3--5 contestants are \textbf{ranked} (1st, 2nd, 3rd, ...).
Let $\pi:\{1,\dots,|A_T|\}\to A_T$ be the observed ranking ($\pi(1)$ is the winner).
This provides \textbf{ordering constraints}:
\begin{equation}
c_{\pi(1),T} > c_{\pi(2),T} > c_{\pi(3),T} > \cdots
\label{eq:finals_order}
\end{equation}

\paragraph{Week classification.}
For each week $t<T$, we classify:
\begin{equation}
\text{WeekType}(t) = 
\begin{cases}
\texttt{normal} & |E_t|=1 \text{ (single elimination)},\\
\texttt{multi} & |E_t|>1 \text{ (double/triple elimination)},\\
\texttt{none} & |E_t|=0 \text{ (no elimination this week)},\\
\texttt{withdrew} & \text{only Withdrew exits, no vote-based elim}.
\end{cases}
\end{equation}

\paragraph{Constraint strength by week type.}
\begin{itemize}
\item \textbf{Normal weeks} ($|E_t|=1$): Strong constraint; eliminated contestant has worst score.
\item \textbf{Multi-elimination weeks} ($|E_t|>1$): All $e\in E_t$ must be in the bottom $|E_t|$ positions.
\item \textbf{No-elimination weeks} ($|E_t|=0$): No constraint on ordering; only regularization applies; \textbf{low certainty}.
\item \textbf{Finals week}: Ordering constraint Eq.~\eqref{eq:finals_order} instead of elimination constraint.
\end{itemize}

For each week $t$, let $E_t\subseteq A_t$ be the observed \emph{vote-determined} eliminated set (excluding Withdrew),
and $S_t=A_t\setminus E_t$ be the survivors.

\subsection{Unknown Fan Votes: Use Vote Shares (Identifiable)}
We estimate fan \textbf{vote shares} rather than absolute votes:
\begin{equation}
v_{i,t}\ge 0,\qquad \sum_{i\in A_t} v_{i,t}=1.
\end{equation}
Absolute votes can be reported as $V_{i,t}=V_t\,v_{i,t}$ for a chosen scale $V_t$ (scale does not affect eliminations).

% ------------------------------------------------
\subsection{Regime A: Percent Seasons (3--27) --- Convex Inverse Voting}
\paragraph{Known: judge shares.}
\begin{equation}
j_{i,t}=\frac{J_{i,t}}{\sum_{k\in A_t} J_{k,t}}.
\end{equation}

\paragraph{Rule: combined score and elimination.}
\begin{equation}
c_{i,t}=j_{i,t}+v_{i,t},
\end{equation}
and eliminated contestants should have the smallest combined scores.

\paragraph{Handling different week types.}
We apply different constraints based on week classification:
\begin{itemize}
\item \textbf{Normal elimination ($|E_t|=1$)}: Single constraint $c_{e,t}\le c_{p,t}+\delta_t$ for all $p\in S_t$.
\item \textbf{Multi-elimination ($|E_t|>1$)}: All eliminated contestants must be in the bottom $|E_t|$:
\begin{equation}
c_{e,t}\le c_{p,t}+\delta_t,\quad \forall e\in E_t,\ \forall p\in S_t.
\end{equation}
\item \textbf{No-elimination ($|E_t|=0$)}: No constraint; only prior regularization applies. Mark as \textbf{low certainty}.
\item \textbf{Withdrew week}: Exclude the withdrew contestant from $E_t$; apply constraints only to remaining $E_t$.
\item \textbf{Finals week ($t=T$)}: Apply ranking constraint Eq.~\eqref{eq:finals_order} instead of elimination.
\end{itemize}

\paragraph{Elimination constraints with weekly slack.}
To handle ties/special episodes/unknown details, we use a weekly slack $\delta_t\ge 0$:
\begin{equation}
c_{e,t}\le c_{p,t}+\delta_t,\qquad \forall e\in E_t,\ \forall p\in S_t.
\label{eq:percent_elim}
\end{equation}
If $E_t=\emptyset$ (no elimination), no constraint is applied (only regularization).

\paragraph{Bayesian formulation with soft constraints.}
We reformulate the problem in a Bayesian framework. Let $\bm v=(v_{i,t})$ denote all vote shares. The posterior is:
\begin{equation}
p(\bm v \mid \text{Elim}) \propto p(\text{Elim} \mid \bm v) \cdot p(\bm v),
\label{eq:bayes}
\end{equation}
where the likelihood encodes elimination constraints and the prior encodes regularization.

\paragraph{Soft elimination likelihood (for normal/multi-elimination weeks).}
Instead of hard constraints, we use a softmax likelihood that the eliminated contestant has the lowest combined score:
\begin{equation}
p(e_t \mid \bm v_t) = \frac{\exp(-\tau \cdot c_{e_t,t})}{\sum_{i\in A_t}\exp(-\tau \cdot c_{i,t})},
\label{eq:soft_elim}
\end{equation}
where $\tau>0$ is a temperature parameter. As $\tau\to\infty$, this approaches a hard constraint.

For no-elimination weeks, we set $p(\text{no elim}\mid \bm v_t)=1$ (uninformative).

\paragraph{Finals likelihood (ranking constraint).}
For the finals week $T$, we use a Plackett-Luce ranking likelihood:
\begin{equation}
p(\pi \mid \bm v_T) = \prod_{k=1}^{|A_T|-1} \frac{\exp(\tau \cdot c_{\pi(k),T})}{\sum_{j=k}^{|A_T|}\exp(\tau \cdot c_{\pi(j),T})},
\label{eq:plackett_luce}
\end{equation}
which assigns higher probability when contestants are ranked in order of their combined scores.

\paragraph{Prior: entropy + temporal smoothness.}
We impose a composite prior:
\begin{equation}
p(\bm v) \propto \exp\left(\alpha\sum_{t,i\in A_t}\mathrm{entr}(v_{i,t}) - \beta\sum_{t=2}^{T}\lVert \bm v_t-\bm v_{t-1}\rVert_{1}\right),
\label{eq:prior}
\end{equation}
where $\mathrm{entr}(x)=-x\log x$ is entropy (favoring uniform distribution when uninformed) and the $L_1$ term encourages temporal stability.

\paragraph{MAP estimation (convex optimization).}
The maximum a posteriori (MAP) estimate maximizes the log-posterior, yielding:
\begin{equation}
\min_{\{v_{i,t}\}}
\quad
-\sum_{t:E_t\neq\emptyset}\log p(e_t\mid \bm v_t)
+\beta\sum_{t=2}^{T}\lVert \bm v_t-\bm v_{t-1}\rVert_{1}
-\alpha\sum_{t=1}^{T}\sum_{i\in A_t}\mathrm{entr}(v_{i,t}),
\label{eq:percent_obj}
\end{equation}
subject to simplex constraints $v_{i,t}\ge 0$, $\sum_{i\in A_t}v_{i,t}=1$.
This is a convex optimization problem solvable by interior-point methods.

% ------------------------------------------------
\subsection{Regime B: Rank Seasons (1--2) --- Integer Inference of Fan Ranks}
Rank seasons use fan \emph{ranks} rather than vote shares. We infer weekly fan ranks and then map to vote shares.

\paragraph{Known: judge ranks.}
Let $r^J_{i,t}$ be the rank of $J_{i,t}$ among $A_t$ (higher score = better rank = lower number; ties use average-rank).

\paragraph{Unknown: fan ranks as a permutation.}
Let $r^F_{i,t}\in\{1,\dots,|A_t|\}$ be the fan rank (1 is best). To enforce a permutation, introduce binary assignment:
\begin{equation}
x_{i,k,t}\in\{0,1\}\quad (i\in A_t,\ k=1,\dots,|A_t|),
\end{equation}
with
\begin{equation}
\sum_{k}x_{i,k,t}=1,\quad \sum_{i\in A_t}x_{i,k,t}=1,\quad
r^F_{i,t}=\sum_{k}k\,x_{i,k,t}.
\label{eq:perm}
\end{equation}

\paragraph{Rule: combined rank and elimination.}
Combined rank-sum (lower is better):
\begin{equation}
c_{i,t}=r^J_{i,t}+r^F_{i,t}.
\end{equation}
Eliminated contestants should have the \emph{worst} (largest) $c_{i,t}$.

\paragraph{Handling different week types (Rank regime).}
\begin{itemize}
\item \textbf{Normal elimination ($|E_t|=1$)}: $c_{e,t}\ge c_{p,t}-\delta_t$ for all $p\in S_t$.
\item \textbf{Multi-elimination ($|E_t|>1$)}: All eliminated contestants in bottom $|E_t|$ by combined rank.
\item \textbf{No-elimination ($|E_t|=0$)}: No constraint; fan ranks regularized toward previous week.
\item \textbf{Withdrew}: Exclude from constraints; assign average rank with high uncertainty.
\item \textbf{Finals ($t=T$)}: Enforce ordering $c_{\pi(1),T}<c_{\pi(2),T}<c_{\pi(3),T}<\cdots$
\end{itemize}

\paragraph{Elimination constraints.}
We allow weekly slack $\delta_t\ge 0$:
\begin{equation}
c_{e,t}\ge c_{p,t}-\delta_t,\qquad \forall e\in E_t,\ \forall p\in S_t.
\label{eq:rank_elim}
\end{equation}

\paragraph{Regularization for unique, realistic ranks.}
We prefer fan ranks that do not jump wildly:
\begin{equation}
\min\ 
M\sum_t \delta_t\ +\ \beta\sum_{t=2}^T \sum_{i\in A_t\cap A_{t-1}} |r^F_{i,t}-r^F_{i,t-1}|.
\label{eq:rank_obj}
\end{equation}
This is an ILP due to binary $x_{i,k,t}$.

\paragraph{Map inferred fan ranks to vote shares.}
To report \emph{fan votes} (shares) consistently across regimes, we map rank to share by a monotone model:
\begin{equation}
v_{i,t}=\frac{\exp(-\lambda(r^F_{i,t}-1))}{\sum_{p\in A_t}\exp(-\lambda(r^F_{p,t}-1))}.
\label{eq:rank_to_share}
\end{equation}
We calibrate $\lambda$ using the distributional shape of vote shares learned from Percent seasons (Regime A),
so that rank-based seasons produce comparable concentration levels.

% ------------------------------------------------
\subsection{Regime C: Seasons 28--34 --- Rank + Bottom2 + Judges' Save}
In this regime, the eliminated contestant must be among the bottom two by combined ranks,
and judges then choose which of the bottom two leaves.

\paragraph{Step 1 (Bottom2 by combined ranks).}
Using the same combined rank $c_{i,t}=r^J_{i,t}+r^F_{i,t}$ (higher value = worse), define bottom-two set $B_t$:
\begin{equation}
|B_t|=2,\quad c_{p,t} \le c_{b,t}+\delta_t,\ \ \forall b\in B_t,\ \forall p\in A_t\setminus B_t.
\label{eq:bottom2}
\end{equation}
This means contestants in the bottom two have combined ranks at least as bad as (or worse than) non-bottom contestants.
Observed elimination $e\in E_t$ must satisfy $e\in B_t$.

\paragraph{Handling special cases in Regime C.}
\begin{itemize}
\item \textbf{No-elimination week ($|E_t|=0$)}: No bottom-2 constraint; only regularization applies.
\item \textbf{Multi-elimination ($|E_t|>1$)}: Extend to bottom-$|E_t|+1$ set (rare in this regime).
\item \textbf{Withdrew}: Contestant exits but is not placed in $B_t$; excluded from all constraints.
\item \textbf{Finals week ($t=T$)}: No bottom-2 mechanism; use ranking constraint Eq.~\eqref{eq:finals_order}.
\item \textbf{Unknown bottom-2 partner}: If only $e$ (eliminated) is known but not $b$ (saved), we enumerate all possible $b\in A_t\setminus\{e\}$.
\end{itemize}

\paragraph{Step 2 (Judges choose within Bottom2).}
Let the other bottom-two contestant be $b$ (saved).
A simple, explainable modeling assumption is that judges are more likely to eliminate the one with lower judges score.
We implement this either as a hard preference
\begin{equation}
J_{e,t}\le J_{b,t},
\label{eq:judge_hard}
\end{equation}
or as a soft penalty (recommended for robustness):
\begin{equation}
\text{penalty}_t=\max\{0,\,J_{e,t}-J_{b,t}\}.
\label{eq:judge_soft}
\end{equation}

\paragraph{Optimization strategy.}
We solve an ILP similar to Regime B with additional constraints Eq.~\eqref{eq:bottom2},
and add the judges-save term (hard or soft). If $b$ is unknown, we enumerate $b\in A_t\setminus\{e\}$,
solve the ILP for each candidate, and pick the solution with minimum objective value.
Finally, map ranks to vote shares by Eq.~\eqref{eq:rank_to_share}.

% ------------------------------------------------
\subsection{Strengthened Constraints for Improved Identifiability}
To reduce the solution space and improve certainty, we add auxiliary constraints derived from observed outcomes.

\paragraph{Finals ranking constraint.}
In the final week, the top 3 (or more) contestants are ranked. Let $\pi$ be the observed final ranking. We enforce:
\begin{equation}
c_{\pi(1),T} > c_{\pi(2),T} > c_{\pi(3),T} > \cdots
\label{eq:finals}
\end{equation}
This provides strong constraints on the final week's vote shares.

\paragraph{Cumulative consistency constraint.}
Contestants who survive longer should, on average, have better cumulative scores. For contestants $i,j$ with $\mathrm{exit}(i)<\mathrm{exit}(j)$:
\begin{equation}
\sum_{t=1}^{\mathrm{exit}(i)} c_{i,t} \le \sum_{t=1}^{\mathrm{exit}(i)} c_{j,t} + \epsilon_{ij},
\label{eq:cumulative}
\end{equation}
where $\epsilon_{ij}\ge 0$ is a slack for occasional upsets.

\paragraph{Transitivity constraint.}
If contestant $i$ is eliminated before $j$, there must exist at least one week where $i$ ranked lower:
\begin{equation}
\mathrm{exit}(i)<\mathrm{exit}(j) \Rightarrow \exists\, t\le\mathrm{exit}(i):\ c_{i,t}<c_{j,t}.
\label{eq:transitive}
\end{equation}

% ------------------------------------------------
\subsection{Consistency Measures (Answering: Does the model produce correct eliminations?)}
Given estimated $\hat v_{i,t}$, we re-compute eliminations under the regime rule and compare with observed $E_t$.

\paragraph{Measure 1: Elimination Accuracy.}
The fraction of weeks where the predicted eliminated contestant matches the actual:
\begin{equation}
\text{Accuracy} = \frac{1}{N_{\text{elim}}}\sum_{t:E_t\neq\emptyset}\mathbf{1}(\hat E_t=E_t).
\label{eq:accuracy}
\end{equation}
A value of 1.0 means perfect consistency; our model targets $\geq 0.95$.

\paragraph{Measure 2: Jaccard Similarity.}
For weeks with multiple eliminations:
\begin{equation}
\text{Jaccard}(t) = \frac{|\hat E_t\cap E_t|}{|\hat E_t\cup E_t|},\qquad
\overline{\text{Jaccard}}=\frac{1}{N_{\text{elim}}}\sum_{t}\text{Jaccard}(t).
\end{equation}

\paragraph{Measure 3: Margin of Victory.}
How ``safely'' the eliminated contestant was identified (larger = more robust):
\begin{equation}
\text{Margin}(t) = |c_{\text{elim},t} - c_{\text{next},t}|,
\end{equation}
where $c_{\text{next},t}$ is the combined score of the contestant closest to elimination.

% ------------------------------------------------
\subsection{Certainty Measures (Answering: How confident are the estimates?)}
The inverse problem admits multiple solutions. We quantify \emph{certainty} for each contestant-week estimate.

\paragraph{Measure 1: Coefficient of Variation (CV).}
\begin{equation}
\text{CV}(i,t) = \frac{\sigma_{i,t}}{\mu_{i,t}},
\end{equation}
where $\mu_{i,t}$ and $\sigma_{i,t}$ are the mean and standard deviation across ensemble solutions.
Lower CV indicates higher certainty. We report CV for each $(i,t)$.

\paragraph{Measure 2: 95\% Confidence Interval Width.}
\begin{equation}
\text{Width}(i,t) = Q_{97.5\%}(\hat v_{i,t}) - Q_{2.5\%}(\hat v_{i,t}).
\end{equation}
Narrower intervals indicate higher certainty.

\paragraph{Measure 3: Entropy of Posterior (for Bayesian approach).}
\begin{equation}
H(i,t) = -\int p(v_{i,t}\mid\text{Elim})\log p(v_{i,t}\mid\text{Elim})\,dv.
\end{equation}
Lower entropy means the posterior is more concentrated (higher certainty).

\paragraph{When is certainty higher or lower?}
\begin{itemize}
\item \textbf{High certainty}: Weeks with elimination events (strong constraints), large margins, fewer active contestants.
\item \textbf{Low certainty}: No-elimination weeks, very close contests, early weeks with many contestants, ambiguous judge scores.
\end{itemize}

% ------------------------------------------------
\subsection{Uncertainty Quantification: Multi-Model Bayesian Ensemble}
The inverse problem admits multiple solutions; we quantify uncertainty through a comprehensive ensemble approach combining multiple strategies.

\subsubsection{Strategy 1: Posterior Sampling (MCMC)}
For Percent seasons, we sample from the full posterior Eq.~\eqref{eq:bayes} using Hamiltonian Monte Carlo (HMC):
\begin{equation}
\{\bm v^{(k)}\}_{k=1}^{K} \sim p(\bm v \mid \text{Elim}).
\end{equation}
This directly provides posterior samples for uncertainty quantification.

\subsubsection{Strategy 2: Multi-Regularization Ensemble}
We solve the optimization under different regularization schemes:
\begin{itemize}
\item \textbf{Model A (Entropy)}: $-\alpha\sum_{t,i}\mathrm{entr}(v_{i,t})$ (maximum entropy, uniform preference).
\item \textbf{Model B (L2-smooth)}: $\beta\sum_t\|\bm v_t-\bm v_{t-1}\|_2^2$ (quadratic temporal smoothness).
\item \textbf{Model C (Sparse)}: $\gamma\sum_{t,i}v_{i,t}^{0.5}$ (promotes vote concentration on fewer contestants).
\end{itemize}
Each model captures different prior beliefs about voting behavior.

\subsubsection{Strategy 3: Perturb-and-Resolve}
For $k=1,\dots,K$:
\begin{itemize}
\item Perturb hyperparameters: $\alpha^{(k)}=\alpha(1+\epsilon_\alpha)$, $\beta^{(k)}=\beta(1+\epsilon_\beta)$, with $\epsilon\sim\mathcal{U}(-0.1,0.1)$.
\item Add noise to judges scores: $J^{(k)}_{i,t}=J_{i,t}+\eta_{i,t}$, $\eta_{i,t}\sim\mathcal{N}(0,0.5^2)$.
\item Re-solve to obtain $\hat v^{(k)}_{i,t}$.
\end{itemize}

\subsubsection{Ensemble Aggregation}
We aggregate solutions using inverse-slack weighting:
\begin{equation}
\hat v_{i,t}^{\text{ens}} = \sum_{m} w_m \hat v_{i,t}^{(m)}, \quad w_m = \frac{\exp(-\lambda \cdot \text{TotalSlack}_m)}{\sum_{m'}\exp(-\lambda \cdot \text{TotalSlack}_{m'})},
\label{eq:ensemble_weight}
\end{equation}
where models with lower slack (better constraint satisfaction) receive higher weights.

\subsubsection{Uncertainty Summaries}
For each contestant-week $(i,t)$, we compute:
\begin{align}
\mu_{i,t} &= \mathbb{E}[v_{i,t}\mid\text{Elim}], \quad
\sigma_{i,t} = \sqrt{\mathrm{Var}[v_{i,t}\mid\text{Elim}]},\\
\text{CI}_{95\%}(i,t) &= \left[Q_{2.5\%},\ Q_{97.5\%}\right] \text{ of posterior/ensemble},\\
\text{Width}(i,t) &= Q_{97.5\%}-Q_{2.5\%},\quad
\text{CV}(i,t)=\sigma_{i,t}/\mu_{i,t}.
\end{align}

\paragraph{Interpretation of uncertainty.}
We expect:
\begin{itemize}
\item \textbf{High certainty}: weeks with elimination (strong constraint) and large margins.
\item \textbf{Low certainty}: no-elimination weeks, close contests, early weeks with many contestants.
\end{itemize}

% ------------------------------------------------
\subsection{Workflow Summary (Algorithm)}
\begin{algorithm}[H]
\caption{Bayesian Inverse Voting with Multi-Model Ensemble (Question 1)}
\begin{algorithmic}[1]
\State \textbf{Data Preparation:} Build $J_{i,t}$, $A_t$, $E_t$ (exclude \texttt{Withdrew} from $E_t$); extract finals rankings.
\For{each season $s$}
    \State \textbf{Phase 1: Point Estimation (MAP)}
    \If{Season in 3--27 (Percent)}
        \State Solve convex program \eqref{eq:percent_obj} with constraints \eqref{eq:finals}, \eqref{eq:cumulative}.
    \ElsIf{Season in 1--2 (Rank)}
        \State Solve ILP \eqref{eq:perm}+\eqref{eq:rank_elim}+\eqref{eq:rank_obj} for fan ranks $\hat{r}^F$.
        \State Map $\hat{r}^F \rightarrow \hat{v}$ by \eqref{eq:rank_to_share}.
    \ElsIf{Season in 28--34 (Rank + Bottom2 + Judges' Save)}
        \State Enumerate bottom-2 partner $b$; solve ILP with \eqref{eq:bottom2} and judges-save penalty.
        \State Map ranks to $\hat{v}$ by \eqref{eq:rank_to_share}.
    \EndIf
    \State
    \State \textbf{Phase 2: Uncertainty Quantification}
    \State Run multi-model ensemble: Models A (entropy), B (L2), C (sparse).
    \State Run perturb-and-resolve ($K=100$ iterations).
    \State Aggregate via Eq.~\eqref{eq:ensemble_weight}; compute $\mu_{i,t}$, $\sigma_{i,t}$, CI$_{95\%}$.
    \State
    \State \textbf{Phase 3: Validation}
    \State Back-substitute to compute $\hat{E}_t$; evaluate accuracy, Jaccard, AUC-ROC, Kendall's $\tau$.
    \State Report CI widths and CV as certainty measures.
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Calculating and Simplifying the Model  }
\lipsum[11]

\section{The Model Results}
\lipsum[6]

\section{Validating the Model}
\lipsum[9]

\section{Conclusions}
\lipsum[6]

\section{A Summary}
\lipsum[6]

\section{Evaluate of the Mode}

\section{Strengths and weaknesses}
\lipsum[12]

\subsection{Strengths}
\begin{itemize}
\item \textbf{Applies widely}\\
This  system can be used for many types of airplanes, and it also
solves the interference during  the procedure of the boarding
airplane,as described above we can get to the  optimization
boarding time.We also know that all the service is automate.
\item \textbf{Improve the quality of the airport service}\\
Balancing the cost of the cost and the benefit, it will bring in
more convenient  for airport and passengers.It also saves many
human resources for the airline.
\end{itemize}

\subsection{How to cite?}
bibliography cite use \cite{1,2,3}

AI cite use \AIcite{AI1,AI2,AI3}

\begin{thebibliography}{99}
\bibitem{1} D.~E. KNUTH   The \TeX{}book  the American
Mathematical Society and Addison-Wesley
Publishing Company , 1984-1986.
\bibitem{2}Lamport, Leslie,  \LaTeX{}: `` A Document Preparation System '',
Addison-Wesley Publishing Company, 1986.
\bibitem{3}\url{https://www.latexstudio.net/}
\end{thebibliography}

\begin{appendices}

\section{First appendix}

In addition, your report must include a letter to the Chief Financial Officer (CFO) of the Goodgrant Foundation, Mr. Alpha Chiang, that describes the optimal investment strategy, your modeling approach and major results, and a brief discussion of your proposed concept of a return-on-investment (ROI). This letter should be no more than two pages in length.

\begin{letter}{Dear, Mr. Alpha Chiang}

\lipsum[1-2]

\vspace{\parskip}

Sincerely yours,

Your friends

\end{letter}
Here are simulation programmes we used in our model as follow.\\

\textbf{\textcolor[rgb]{0.98,0.00,0.00}{Input matlab source:}}
\lstinputlisting[language=Matlab]{./code/mcmthesis-matlab1.m}

\section{Second appendix}

some more text \textcolor[rgb]{0.98,0.00,0.00}{\textbf{Input C++ source:}}
\lstinputlisting[language=C++]{./code/mcmthesis-sudoku.cpp}

\end{appendices}


\AImatter

\begin{ReportAiUse}{9}
\bibitem{AI1}
Bing AI\\
Query1: write a sort algorithm\\
Output: Sure, I can help you with that. Here is a simple implementation of the selection sort algorithm in Python:
\begin{lstlisting}[language=python]
def selection_sort(arr):
    for i in range(len(arr)):
        min_idx = i
        for j in range(i + 1, len(arr)):
            if arr[min_idx] > arr[j]:
                min_idx = j
        arr[i], arr[min_idx] = arr[min_idx], arr[i]
    return arr
\end{lstlisting}
This algorithm sorts an array by repeatedly finding the minimum element from the unsorted part of the array and putting it at the beginning. \href{https://www.geeksforgeeks.org/sorting-algorithms/}{The time complexity of this algorithm is O($n^2$)}.

There are many other sorting algorithms available such as bubble sort, insertion sort, merge sort, quick sort, heap sort, and more. Each of these algorithms has its own advantages and disadvantages. You can learn more about them on GeeksforGeeks and Programiz.

I hope this helps!

\bibitem{AI2}
OpenAl Enie (Nov 5, 2023 version, Ernie 4.0)\\
Query1: <insert the exact wording of any subsequent input into the Al tool>\\
Output: <insert the complete output from the second query>

\bibitem{AI3}
Github CoPilot (Feb 3, 2024 version)\\
Query1: <insert the exact wording you input into the Al tool>\\
Output: <insert the complete output fron the Al tool>

\bibitem{AI4}
Google Bard (Feb 2,2024 version)\\
Query: <insert the exact wording of your query>\\
Output: <irsert the complete output from the Al tool>

\bibitem{AI5}
Bing AI\\
Query: write a FFT algorithm\\
Output: Sure, here is a Python implementation of the Fast Fourier Transform (FFT) algorithm:
\begin{lstlisting}[language=python]
import numpy as np

def fft(x):
    n = len(x)
    if n == 1:
        return x
    even = fft(x[0::2])
    odd = fft(x[1::2])
    T = [np.exp(-2j * np.pi * k / n) * odd[k] for k in range(n // 2)]
    return [even[k] + T[k] for k in range(n // 2)] + [even[k] - T[k] for k in range(n // 2)]
\end{lstlisting}
The FFT is a widely used algorithm for computing the \textbf{Discrete Fourier Transform (DFT)} of a sequence. It reduces the number of computations needed from O(n$^2$) to O(n log n) 12. The basic idea of the FFT is to apply divide and conquer. We divide the coefficient vector of the polynomial into two vectors, recursively compute the DFT for each of them, and combine the results to compute the DFT of the complete polynomial 3.

I hope this helps!

\end{ReportAiUse}

\end{document}
%% 
%% This work consists of these files mcmthesis.dtx,
%%                                   figures/ and
%%                                   code/,
%% and the derived files             mcmthesis.cls,
%%                                   mcmthesis-demo.tex,
%%                                   README,
%%                                   LICENSE,
%%                                   mcmthesis.pdf and
%%                                   mcmthesis-demo.pdf.
%%
%% End of file `mcmthesis-demo.tex'.
