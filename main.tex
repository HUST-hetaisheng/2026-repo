%%
%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%% !Mode:: "TeX:UTF-8"
%% -----------------------------------
%% This is a generated file.
%% 
%% Copyright (C) 2010 -- 2015 by latexstudio
%%       2014 -- 2019 by Liam Huang
%%       2019 -- present by latexstudio.net
%% 
%% License: The LaTeX Project Public License 1.3c
%% 
%% The Current Maintainer of this work is latexstudio.net.
%% 
\documentclass{mcmthesis}
 %\documentclass[CTeX = true]{mcmthesis}  % 当使用 CTeX 套装时请注释上一行使用该行的设置
\mcmsetup{tstyle=\color{red}\bfseries,%修改题号，队号的颜色和加粗显示，黑色可以修改为 black
        tcn = 2607256, problem = C, %修改队号，参赛题号
        sheet = true, titleinsheet = false, keywordsinsheet = true,%修改sheet显示信息
        titlepage = false, abstract = true}

  %四款字体可以选择
  %\usepackage{times}
  \usepackage{newtxtext,newtxmath} %CTeX 无此字体，可用 txfonts 替代，请使用新版 TeXLive.
  %\usepackage{palatino}
  %\usepackage{txfonts}

\usepackage{indentfirst}  %首行缩进，注释掉，首行就不再缩进。
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\title{The \LaTeX{} Template for MCM Version \MCMversion}
\author{\small \href{https://www.latexstudio.net/}
  {\includegraphics[width=7cm]{mcmthesis-logo}}}
\date{\today}
\begin{document}
\begin{abstract}
\par Use this template to begin typing the first page (summary page) of your electronic report. This
template uses a 12-point Times New Roman font. Submit your paper as an Adobe PDF
electronic file (e.g. 1111111.pdf), typed in English, with a readable font of at least 12-point type. 

Do not include the name of your school, advisor, or team members on this or any page. 

Be sure to change the control number and problem choice above. 

You may delete these instructions as you begin to type your report here.  

\textbf{Follow us @COMAPMath on Twitter or COMAPCHINAOFFICIAL on Weibo for the
most up to date contest information.}

\begin{keywords}
keyword1; keyword2
\end{keywords}
\end{abstract}
\maketitle
%% Generate the Table of Contents, if it's needed.
\tableofcontents
\newpage
%%
%% Generate the Memorandum, if it's needed.
%% \memoto{\LaTeX{}studio}
%% \memofrom{Liam Huang}
%% \memosubject{Happy \TeX{}ing!}
%% \memodate{\today}
%% \memologo{\LARGE I'm pretending to be a LOGO!}
%% \begin{memo}[Memorandum]
%%   \lipsum[1-3]
%% \end{memo}
%%
\section{Introduction}
\subsection{Background}
Variety entertainment has become an indispensable spice of modern life, with various emerging shows continuing to emerge, and competition in the global variety entertainment industry has become increasingly fierce. In an era of such frequent iteration of variety IPs, Dancing with the Stars (DWTS), the American version of an international television franchise, has firmly established itself. Having completed 34 seasons with audiences spanning numerous countries worldwide, it has evolved into a benchmark variety show renowned for both its reputation and popularity.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/dwtc背景图.png}
    \caption{Dancing with the Stars poster season33}
    \label{fig:placeholder}
\end{figure}
DWTS features celebrity contestants paired with professional dancers, who compete through weekly dance performances. Elimination and advancement results are determined by combining professional judges' scores and audience votes. Since judges' judgment criteria for dance technical proficiency are often subjective, and fan votes are susceptible to non-technical factors such as celebrity popularity and personal charisma, the final results of the competition have always been mired in constant discussions and controversies—even as the program team has repeatedly attempted new methods of integrating judges' scores and fan votes.

In recent years, as audiences have paid increasing attention to the fairness of competitive variety shows, DWTS is in urgent need of a fair, impartial, and effective scoring mechanism. This mechanism must ensure the program's viewership and entertainment value while maintaining the strict confidentiality of fan votes, thereby achieving the dual guarantee of professionalism and popularity. This will serve as a key measure for DWTS to attract audiences and continuously enhance its IP influence in future seasons.
\subsection{Restatement of the Problem}

Follow our WeChat official account for more LaTeX materials and information.

\centerline{\includegraphics[width=5cm]{qrcodewechat}}

\subsection{Syntax (how to type \LaTeX\ commands --- these
  are the rules)}

\lipsum[3]
\begin{itemize}
\item the angular velocity of the bat,
\item the velocity of the ball, and
\item the position of impact along the bat.
\end{itemize}
\lipsum[4]
\emph{center of percussion} [Brody 1986], \lipsum[5]

\begin{Theorem} \label{thm:latex}
\LaTeX
\end{Theorem}
\begin{Lemma} \label{thm:tex}
\TeX .
\end{Lemma}
\begin{proof}
The proof of theorem.
\end{proof}

\subsection{Other Assumptions}
\lipsum[6]
\begin{itemize}
\item
\item
\item
\item
\end{itemize}

\lipsum[7]

\section{Analysis of the Problem}
\begin{figure}[h]
\small
\centering
\includegraphics[width=8cm]{example-image-a}
\caption{The name of figure} \label{fig:aa}
\end{figure}

\lipsum[8] \eqref{aa}
\begin{equation}
a^2 \label{aa}
\end{equation}

\[
  \begin{pmatrix}{*{20}c}
  {a_{11} } & {a_{12} } & {a_{13} }  \\
  {a_{21} } & {a_{22} } & {a_{23} }  \\
  {a_{31} } & {a_{32} } & {a_{33} }  \\
  \end{pmatrix}
  = \frac{{Opposite}}{{Hypotenuse}}\cos ^{ - 1} \theta \arcsin \theta
\]
\lipsum[9]

\[
  p_{j}=\begin{cases} 0,&\text{if $j$ is odd}\\
  r!\,(-1)^{j/2},&\text{if $j$ is even}
  \end{cases}
\]

\lipsum[10]

\[
  \arcsin \theta  =
  \mathop{{\int\!\!\!\!\!\int\!\!\!\!\!\int}} \limits_\varphi
  {\mathop {\lim }\limits_{x \to \infty } \frac{{n!}}{{r!\left( {n - r}
  \right)!}}} \eqno (1)
\]
% ============================================
% Full modeling for Question 1 (all seasons)
% ============================================

\section{Model 1: Bayesian Inverse Estimation of Fan Votes}

\subsection{Problem Formulation: An Inverse Problem}
Estimating fan votes from observed eliminations is an \textbf{inverse problem}: given the output (who was eliminated), we infer the hidden input (fan votes). This problem is inherently \emph{under-determined}---many vote distributions can produce the same elimination outcome. Our approach addresses this by:
\begin{enumerate}
\item Formulating season-specific elimination rules as probabilistic constraints;
\item Incorporating temporal smoothness and maximum-entropy priors to regularize the solution space;
\item Quantifying uncertainty through Bayesian posterior inference.
\end{enumerate}

\subsection{Season Rules (Three Regimes)}
The show combines judges' scores and fan votes using different rules across seasons:
\begin{itemize}
\item \textbf{Seasons 1--2 (Rank)}: combine by ranks (judge rank + fan rank); the worst sum is eliminated.
\item \textbf{Seasons 3--27 (Percent)}: combine by shares (judge score share + fan vote share); the lowest sum is eliminated.
\item \textbf{Seasons 28--34 (Rank + Bottom2 + Judges' Save)}: identify bottom two by combined ranks, then judges choose one of them to eliminate.
\end{itemize}
Our goal is not to predict eliminations directly, but to \emph{infer fan votes} that make the observed eliminations consistent with the rule in that season.

\subsection{Weekly Data Construction}
Let $J_{i,t}$ be contestant $i$'s total judges score in week $t$ (sum over available judges; missing values ignored).
We define the active set in week $t$:
\begin{equation}
A_t=\{i: J_{i,t}>0\},
\end{equation}
since the dataset records $0$ after a contestant exits. Let $T$ be the last week of the season:
\begin{equation}
T=\max\{t:\exists i,\ J_{i,t}>0\}.
\end{equation}
We determine each contestant's last active week
\begin{equation}
\mathrm{exit}(i)=\max\{t\le T: J_{i,t}>0\}.
\end{equation}
We treat \texttt{Withdrew} as exogenous attrition (not vote-determined): it contributes to $A_t$ before exit,
but is excluded from vote-based elimination constraints.

For each week $t$, let $E_t\subseteq A_t$ be the observed \emph{vote-determined} eliminated set (possibly multiple),
and $S_t=A_t\setminus E_t$ be the survivors.

\subsection{Unknown Fan Votes: Use Vote Shares (Identifiable)}
We estimate fan \textbf{vote shares} rather than absolute votes:
\begin{equation}
v_{i,t}\ge 0,\qquad \sum_{i\in A_t} v_{i,t}=1.
\end{equation}
Absolute votes can be reported as $V_{i,t}=V_t\,v_{i,t}$ for a chosen scale $V_t$ (scale does not affect eliminations).

% ------------------------------------------------
\subsection{Regime A: Percent Seasons (3--27) --- Convex Inverse Voting}
\paragraph{Known: judge shares.}
\begin{equation}
j_{i,t}=\frac{J_{i,t}}{\sum_{k\in A_t} J_{k,t}}.
\end{equation}

\paragraph{Rule: combined score and elimination.}
\begin{equation}
c_{i,t}=j_{i,t}+v_{i,t},
\end{equation}
and eliminated contestants should have the smallest combined scores.

\paragraph{Elimination constraints with weekly slack.}
To handle ties/special episodes/unknown details, we use a weekly slack $\delta_t\ge 0$:
\begin{equation}
c_{e,t}\le c_{p,t}+\delta_t,\qquad \forall e\in E_t,\ \forall p\in S_t.
\label{eq:percent_elim}
\end{equation}
If $E_t=\emptyset$ (no elimination), we enforce $\delta_t=0$.

\paragraph{Bayesian formulation with soft constraints.}
We reformulate the problem in a Bayesian framework. Let $\bm v=(v_{i,t})$ denote all vote shares. The posterior is:
\begin{equation}
p(\bm v \mid \text{Elim}) \propto p(\text{Elim} \mid \bm v) \cdot p(\bm v),
\label{eq:bayes}
\end{equation}
where the likelihood encodes elimination constraints and the prior encodes regularization.

\paragraph{Soft elimination likelihood.}
Instead of hard constraints, we use a softmax likelihood that the eliminated contestant has the lowest combined score:
\begin{equation}
p(e_t \mid \bm v_t) = \frac{\exp(-\tau \cdot c_{e_t,t})}{\sum_{i\in A_t}\exp(-\tau \cdot c_{i,t})},
\label{eq:soft_elim}
\end{equation}
where $\tau>0$ is a temperature parameter. As $\tau\to\infty$, this approaches a hard constraint.

\paragraph{Prior: entropy + temporal smoothness.}
We impose a composite prior:
\begin{equation}
p(\bm v) \propto \exp\left(\alpha\sum_{t,i\in A_t}\mathrm{entr}(v_{i,t}) - \beta\sum_{t=2}^{T}\lVert \bm v_t-\bm v_{t-1}\rVert_{1}\right),
\label{eq:prior}
\end{equation}
where $\mathrm{entr}(x)=-x\log x$ is entropy (favoring uniform distribution when uninformed) and the $L_1$ term encourages temporal stability.

\paragraph{MAP estimation (convex optimization).}
The maximum a posteriori (MAP) estimate maximizes the log-posterior, yielding:
\begin{equation}
\min_{\{v_{i,t}\}}
\quad
-\sum_{t:E_t\neq\emptyset}\log p(e_t\mid \bm v_t)
+\beta\sum_{t=2}^{T}\lVert \bm v_t-\bm v_{t-1}\rVert_{1}
-\alpha\sum_{t=1}^{T}\sum_{i\in A_t}\mathrm{entr}(v_{i,t}),
\label{eq:percent_obj}
\end{equation}
subject to simplex constraints $v_{i,t}\ge 0$, $\sum_{i\in A_t}v_{i,t}=1$.
This is a convex optimization problem solvable by interior-point methods.

% ------------------------------------------------
\subsection{Regime B: Rank Seasons (1--2) --- Integer Inference of Fan Ranks}
Rank seasons use fan \emph{ranks} rather than vote shares. We infer weekly fan ranks and then map to vote shares.

\paragraph{Known: judge ranks.}
Let $r^J_{i,t}$ be the rank of $J_{i,t}$ among $A_t$ (higher score = better rank; ties use average-rank).

\paragraph{Unknown: fan ranks as a permutation.}
Let $r^F_{i,t}\in\{1,\dots,|A_t|\}$ be the fan rank (1 is best). To enforce a permutation, introduce binary assignment:
\begin{equation}
x_{i,k,t}\in\{0,1\}\quad (i\in A_t,\ k=1,\dots,|A_t|),
\end{equation}
with
\begin{equation}
\sum_{k}x_{i,k,t}=1,\quad \sum_{i\in A_t}x_{i,k,t}=1,\quad
r^F_{i,t}=\sum_{k}k\,x_{i,k,t}.
\label{eq:perm}
\end{equation}

\paragraph{Rule: combined rank and elimination.}
Combined rank-sum:
\begin{equation}
c_{i,t}=r^J_{i,t}+r^F_{i,t}.
\end{equation}
Eliminated contestants should have the \emph{worst} (largest) $c_{i,t}$.
We again allow weekly slack $\delta_t\ge 0$:
\begin{equation}
c_{e,t}\ge c_{p,t}-\delta_t,\qquad \forall e\in E_t,\ \forall p\in S_t.
\label{eq:rank_elim}
\end{equation}

\paragraph{Regularization for unique, realistic ranks.}
We prefer fan ranks that do not jump wildly:
\begin{equation}
\min\ 
M\sum_t \delta_t\ +\ \beta\sum_{t=2}^T \sum_{i\in A_t\cap A_{t-1}} |r^F_{i,t}-r^F_{i,t-1}|.
\label{eq:rank_obj}
\end{equation}
This is an ILP due to binary $x_{i,k,t}$.

\paragraph{Map inferred fan ranks to vote shares.}
To report \emph{fan votes} (shares) consistently across regimes, we map rank to share by a monotone model:
\begin{equation}
v_{i,t}=\frac{\exp(-\lambda(r^F_{i,t}-1))}{\sum_{p\in A_t}\exp(-\lambda(r^F_{p,t}-1))}.
\label{eq:rank_to_share}
\end{equation}
We calibrate $\lambda$ using the distributional shape of vote shares learned from Percent seasons (Regime A),
so that rank-based seasons produce comparable concentration levels.

% ------------------------------------------------
\subsection{Regime C: Seasons 28--34 --- Rank + Bottom2 + Judges' Save}
In this regime, the eliminated contestant must be among the bottom two by combined ranks,
and judges then choose which of the bottom two leaves.

\paragraph{Step 1 (Bottom2 by combined ranks).}
Using the same combined rank $c_{i,t}=r^J_{i,t}+r^F_{i,t}$, define bottom-two set $B_t$:
\begin{equation}
|B_t|=2,\quad c_{b,t}\ge c_{p,t}-\delta_t,\ \ \forall b\in B_t,\ \forall p\in A_t\setminus B_t.
\label{eq:bottom2}
\end{equation}
Observed elimination $e\in E_t$ must satisfy $e\in B_t$.

\paragraph{Step 2 (Judges choose within Bottom2).}
Let the other bottom-two contestant be $b$ (saved).
A simple, explainable modeling assumption is that judges are more likely to eliminate the one with lower judges score.
We implement this either as a hard preference
\begin{equation}
J_{e,t}\le J_{b,t},
\label{eq:judge_hard}
\end{equation}
or as a soft penalty (recommended for robustness):
\begin{equation}
\text{penalty}_t=\max\{0,\,J_{e,t}-J_{b,t}\}.
\label{eq:judge_soft}
\end{equation}

\paragraph{Optimization strategy.}
We solve an ILP similar to Regime B with additional constraints Eq.~\eqref{eq:bottom2},
and add the judges-save term (hard or soft). If $b$ is unknown, we enumerate $b\in A_t\setminus\{e\}$,
solve the ILP for each candidate, and pick the solution with minimum objective value.
Finally, map ranks to vote shares by Eq.~\eqref{eq:rank_to_share}.

% ------------------------------------------------
\subsection{Strengthened Constraints for Improved Identifiability}
To reduce the solution space and improve certainty, we add auxiliary constraints derived from observed outcomes.

\paragraph{Finals ranking constraint.}
In the final week, the top 3 (or more) contestants are ranked. Let $\pi$ be the observed final ranking. We enforce:
\begin{equation}
c_{\pi(1),T} > c_{\pi(2),T} > c_{\pi(3),T} > \cdots
\label{eq:finals}
\end{equation}
This provides strong constraints on the final week's vote shares.

\paragraph{Cumulative consistency constraint.}
Contestants who survive longer should, on average, have better cumulative scores. For contestants $i,j$ with $\mathrm{exit}(i)<\mathrm{exit}(j)$:
\begin{equation}
\sum_{t=1}^{\mathrm{exit}(i)} c_{i,t} \le \sum_{t=1}^{\mathrm{exit}(i)} c_{j,t} + \epsilon_{ij},
\label{eq:cumulative}
\end{equation}
where $\epsilon_{ij}\ge 0$ is a slack for occasional upsets.

\paragraph{Transitivity constraint.}
If contestant $i$ is eliminated before $j$, there must exist at least one week where $i$ ranked lower:
\begin{equation}
\mathrm{exit}(i)<\mathrm{exit}(j) \Rightarrow \exists\, t\le\mathrm{exit}(i):\ c_{i,t}<c_{j,t}.
\label{eq:transitive}
\end{equation}

% ------------------------------------------------
\subsection{Consistency Evaluation Metrics}
Given estimated $\hat v_{i,t}$, we re-compute eliminations under the regime rule and compare with observed $E_t$.
We report multiple metrics:

\paragraph{Primary metrics.}
\begin{align}
\text{Exact-set accuracy} &= \frac{1}{N_{\text{elim}}}\sum_{t:E_t\neq\emptyset}\mathbf{1}(\hat E_t=E_t),\\
\text{Jaccard}(t) &= \frac{|\hat E_t\cap E_t|}{|\hat E_t\cup E_t|},\qquad
\overline{\text{Jaccard}}=\text{mean}_t\,\text{Jaccard}(t),\\
\text{Margin}(t) &=
\begin{cases}
\hat c_{(m+1),t}-\hat c_{(m),t}, & \text{Percent (smallest eliminated)}\\
\hat c_{(n),t}-\hat c_{(n-1),t}, & \text{Rank (largest eliminated)}
\end{cases}
\end{align}
where $m=|E_t|$ and order statistics are computed among active contestants.

\paragraph{Additional robustness metrics.}
\begin{align}
\text{AUC-ROC} &= \text{Area under ROC curve for predicting elimination probability},\\
\tau_K &= \text{Kendall's } \tau \text{ between predicted and observed weekly rankings}.
\end{align}

% ------------------------------------------------
\subsection{Uncertainty Quantification: Multi-Model Bayesian Ensemble}
The inverse problem admits multiple solutions; we quantify uncertainty through a comprehensive ensemble approach combining multiple strategies.

\subsubsection{Strategy 1: Posterior Sampling (MCMC)}
For Percent seasons, we sample from the full posterior Eq.~\eqref{eq:bayes} using Hamiltonian Monte Carlo (HMC):
\begin{equation}
\{\bm v^{(k)}\}_{k=1}^{K} \sim p(\bm v \mid \text{Elim}).
\end{equation}
This directly provides posterior samples for uncertainty quantification.

\subsubsection{Strategy 2: Multi-Regularization Ensemble}
We solve the optimization under different regularization schemes:
\begin{itemize}
\item \textbf{Model A (Entropy)}: $-\alpha\sum_{t,i}\mathrm{entr}(v_{i,t})$ (maximum entropy, uniform preference).
\item \textbf{Model B (L2-smooth)}: $\beta\sum_t\|\bm v_t-\bm v_{t-1}\|_2^2$ (quadratic temporal smoothness).
\item \textbf{Model C (Sparse)}: $\gamma\sum_{t,i}v_{i,t}^{0.5}$ (promotes vote concentration on fewer contestants).
\end{itemize}
Each model captures different prior beliefs about voting behavior.

\subsubsection{Strategy 3: Perturb-and-Resolve}
For $k=1,\dots,K$:
\begin{itemize}
\item Perturb hyperparameters: $\alpha^{(k)}=\alpha(1+\epsilon_\alpha)$, $\beta^{(k)}=\beta(1+\epsilon_\beta)$, with $\epsilon\sim\mathcal{U}(-0.1,0.1)$.
\item Add noise to judges scores: $J^{(k)}_{i,t}=J_{i,t}+\eta_{i,t}$, $\eta_{i,t}\sim\mathcal{N}(0,0.5^2)$.
\item Re-solve to obtain $\hat v^{(k)}_{i,t}$.
\end{itemize}

\subsubsection{Ensemble Aggregation}
We aggregate solutions using inverse-slack weighting:
\begin{equation}
\hat v_{i,t}^{\text{ens}} = \sum_{m} w_m \hat v_{i,t}^{(m)}, \quad w_m = \frac{\exp(-\lambda \cdot \text{TotalSlack}_m)}{\sum_{m'}\exp(-\lambda \cdot \text{TotalSlack}_{m'})},
\label{eq:ensemble_weight}
\end{equation}
where models with lower slack (better constraint satisfaction) receive higher weights.

\subsubsection{Uncertainty Summaries}
For each contestant-week $(i,t)$, we compute:
\begin{align}
\mu_{i,t} &= \mathbb{E}[v_{i,t}\mid\text{Elim}], \quad
\sigma_{i,t} = \sqrt{\mathrm{Var}[v_{i,t}\mid\text{Elim}]},\\
\text{CI}_{95\%}(i,t) &= \left[Q_{2.5\%},\ Q_{97.5\%}\right] \text{ of posterior/ensemble},\\
\text{Width}(i,t) &= Q_{97.5\%}-Q_{2.5\%},\quad
\text{CV}(i,t)=\sigma_{i,t}/\mu_{i,t}.
\end{align}

\paragraph{Interpretation of uncertainty.}
We expect:
\begin{itemize}
\item \textbf{High certainty}: weeks with elimination (strong constraint) and large margins.
\item \textbf{Low certainty}: no-elimination weeks, close contests, early weeks with many contestants.
\end{itemize}

% ------------------------------------------------
\subsection{Workflow Summary (Algorithm)}
\begin{algorithm}[H]
\caption{Bayesian Inverse Voting with Multi-Model Ensemble (Question 1)}
\begin{algorithmic}[1]
\State \textbf{Data Preparation:} Build $J_{i,t}$, $A_t$, $E_t$ (exclude \texttt{Withdrew} from $E_t$); extract finals rankings.
\For{each season $s$}
    \State \textbf{Phase 1: Point Estimation (MAP)}
    \If{Season in 3--27 (Percent)}
        \State Solve convex program \eqref{eq:percent_obj} with constraints \eqref{eq:finals}, \eqref{eq:cumulative}.
    \ElsIf{Season in 1--2 (Rank)}
        \State Solve ILP \eqref{eq:perm}+\eqref{eq:rank_elim}+\eqref{eq:rank_obj} for fan ranks $\hat{r}^F$.
        \State Map $\hat{r}^F \rightarrow \hat{v}$ by \eqref{eq:rank_to_share}.
    \ElsIf{Season in 28--34 (Rank + Bottom2 + Judges' Save)}
        \State Enumerate bottom-2 partner $b$; solve ILP with \eqref{eq:bottom2} and judges-save penalty.
        \State Map ranks to $\hat{v}$ by \eqref{eq:rank_to_share}.
    \EndIf
    \State
    \State \textbf{Phase 2: Uncertainty Quantification}
    \State Run multi-model ensemble: Models A (entropy), B (L2), C (sparse).
    \State Run perturb-and-resolve ($K=100$ iterations).
    \State Aggregate via Eq.~\eqref{eq:ensemble_weight}; compute $\mu_{i,t}$, $\sigma_{i,t}$, CI$_{95\%}$.
    \State
    \State \textbf{Phase 3: Validation}
    \State Back-substitute to compute $\hat{E}_t$; evaluate accuracy, Jaccard, AUC-ROC, Kendall's $\tau$.
    \State Report CI widths and CV as certainty measures.
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Calculating and Simplifying the Model  }
\lipsum[11]

\section{The Model Results}
\lipsum[6]

\section{Validating the Model}
\lipsum[9]

\section{Conclusions}
\lipsum[6]

\section{A Summary}
\lipsum[6]

\section{Evaluate of the Mode}

\section{Strengths and weaknesses}
\lipsum[12]

\subsection{Strengths}
\begin{itemize}
\item \textbf{Applies widely}\\
This  system can be used for many types of airplanes, and it also
solves the interference during  the procedure of the boarding
airplane,as described above we can get to the  optimization
boarding time.We also know that all the service is automate.
\item \textbf{Improve the quality of the airport service}\\
Balancing the cost of the cost and the benefit, it will bring in
more convenient  for airport and passengers.It also saves many
human resources for the airline.
\end{itemize}

\subsection{How to cite?}
bibliography cite use \cite{1,2,3}

AI cite use \AIcite{AI1,AI2,AI3}

\begin{thebibliography}{99}
\bibitem{1} D.~E. KNUTH   The \TeX{}book  the American
Mathematical Society and Addison-Wesley
Publishing Company , 1984-1986.
\bibitem{2}Lamport, Leslie,  \LaTeX{}: `` A Document Preparation System '',
Addison-Wesley Publishing Company, 1986.
\bibitem{3}\url{https://www.latexstudio.net/}
\end{thebibliography}

\begin{appendices}

\section{First appendix}

In addition, your report must include a letter to the Chief Financial Officer (CFO) of the Goodgrant Foundation, Mr. Alpha Chiang, that describes the optimal investment strategy, your modeling approach and major results, and a brief discussion of your proposed concept of a return-on-investment (ROI). This letter should be no more than two pages in length.

\begin{letter}{Dear, Mr. Alpha Chiang}

\lipsum[1-2]

\vspace{\parskip}

Sincerely yours,

Your friends

\end{letter}
Here are simulation programmes we used in our model as follow.\\

\textbf{\textcolor[rgb]{0.98,0.00,0.00}{Input matlab source:}}
\lstinputlisting[language=Matlab]{./code/mcmthesis-matlab1.m}

\section{Second appendix}

some more text \textcolor[rgb]{0.98,0.00,0.00}{\textbf{Input C++ source:}}
\lstinputlisting[language=C++]{./code/mcmthesis-sudoku.cpp}

\end{appendices}


\AImatter

\begin{ReportAiUse}{9}
\bibitem{AI1}
Bing AI\\
Query1: write a sort algorithm\\
Output: Sure, I can help you with that. Here is a simple implementation of the selection sort algorithm in Python:
\begin{lstlisting}[language=python]
def selection_sort(arr):
    for i in range(len(arr)):
        min_idx = i
        for j in range(i + 1, len(arr)):
            if arr[min_idx] > arr[j]:
                min_idx = j
        arr[i], arr[min_idx] = arr[min_idx], arr[i]
    return arr
\end{lstlisting}
This algorithm sorts an array by repeatedly finding the minimum element from the unsorted part of the array and putting it at the beginning. \href{https://www.geeksforgeeks.org/sorting-algorithms/}{The time complexity of this algorithm is O($n^2$)}.

There are many other sorting algorithms available such as bubble sort, insertion sort, merge sort, quick sort, heap sort, and more. Each of these algorithms has its own advantages and disadvantages. You can learn more about them on GeeksforGeeks and Programiz.

I hope this helps!

\bibitem{AI2}
OpenAl Enie (Nov 5, 2023 version, Ernie 4.0)\\
Query1: <insert the exact wording of any subsequent input into the Al tool>\\
Output: <insert the complete output from the second query>

\bibitem{AI3}
Github CoPilot (Feb 3, 2024 version)\\
Query1: <insert the exact wording you input into the Al tool>\\
Output: <insert the complete output fron the Al tool>

\bibitem{AI4}
Google Bard (Feb 2,2024 version)\\
Query: <insert the exact wording of your query>\\
Output: <irsert the complete output from the Al tool>

\bibitem{AI5}
Bing AI\\
Query: write a FFT algorithm\\
Output: Sure, here is a Python implementation of the Fast Fourier Transform (FFT) algorithm:
\begin{lstlisting}[language=python]
import numpy as np

def fft(x):
    n = len(x)
    if n == 1:
        return x
    even = fft(x[0::2])
    odd = fft(x[1::2])
    T = [np.exp(-2j * np.pi * k / n) * odd[k] for k in range(n // 2)]
    return [even[k] + T[k] for k in range(n // 2)] + [even[k] - T[k] for k in range(n // 2)]
\end{lstlisting}
The FFT is a widely used algorithm for computing the \textbf{Discrete Fourier Transform (DFT)} of a sequence. It reduces the number of computations needed from O(n$^2$) to O(n log n) 12. The basic idea of the FFT is to apply divide and conquer. We divide the coefficient vector of the polynomial into two vectors, recursively compute the DFT for each of them, and combine the results to compute the DFT of the complete polynomial 3.

I hope this helps!

\end{ReportAiUse}

\end{document}
%% 
%% This work consists of these files mcmthesis.dtx,
%%                                   figures/ and
%%                                   code/,
%% and the derived files             mcmthesis.cls,
%%                                   mcmthesis-demo.tex,
%%                                   README,
%%                                   LICENSE,
%%                                   mcmthesis.pdf and
%%                                   mcmthesis-demo.pdf.
%%
%% End of file `mcmthesis-demo.tex'.
